{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 觀察topic用字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from enum import Enum\n",
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from gensim import corpora, models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自定義 data types and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContentType(Enum):\n",
    "    TIT = 'title'\n",
    "    ABS = 'abstract'\n",
    "    AUT = 'author'\n",
    "    SEC = 'section'\n",
    "    \n",
    "def get_contents(content_type):\n",
    "    all_contents = []\n",
    "    dataset_path = '../dataset'\n",
    "    for file in os.listdir(dataset_path):\n",
    "        file_path = os.path.join(dataset_path, file)\n",
    "        if os.path.isfile(file_path):\n",
    "            with open(file_path) as f:  \n",
    "                line = f.readlines()\n",
    "                if content_type == ContentType.AUT:\n",
    "                    line = line[1]\n",
    "                elif content_type == ContentType.SEC:\n",
    "                    line = line[2]\n",
    "                elif content_type == ContentType.ABS:\n",
    "                    line = line[3]\n",
    "                else:\n",
    "                    line = line[0]\n",
    "                line = line.strip()\n",
    "                all_contents.append(line)\n",
    "        else:\n",
    "            print(file_path + ' does not exist.')\n",
    "    return all_contents\n",
    "\n",
    "\n",
    "def get_all_titles():\n",
    "    return get_contents(ContentType.TIT)\n",
    "\n",
    "def get_all_authors():        \n",
    "    return get_contents(ContentType.AUT)\n",
    "\n",
    "def get_all_sections():\n",
    "    return get_contents(ContentType.SEC)\n",
    "\n",
    "def get_all_abstracts():\n",
    "    return get_contents(ContentType.ABS)\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            token = wordnet_lemmatizer.lemmatize(token, pos='v')\n",
    "            token = wordnet_lemmatizer.lemmatize(token, pos='n')\n",
    "            result.append(token)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 取出所有摘要"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "共 1343 篇論文\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>We consider the problem of actively eliciting ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>We investigate the task of distractor generati...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The most common representation formalisms for ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Statistical relational learning models are pow...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Multimodal representation learning is gaining ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Reinforcement learning (RL) has shown its adva...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Selecting appropriate tutoring help actions th...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Recognizing time expressions is a fundamental ...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>When facing large-scale image datasets, online...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Temporal modeling in videos is a fundamental y...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            abstract  index\n",
       "0  We consider the problem of actively eliciting ...      0\n",
       "1  We investigate the task of distractor generati...      1\n",
       "2  The most common representation formalisms for ...      2\n",
       "3  Statistical relational learning models are pow...      3\n",
       "4  Multimodal representation learning is gaining ...      4\n",
       "5  Reinforcement learning (RL) has shown its adva...      5\n",
       "6  Selecting appropriate tutoring help actions th...      6\n",
       "7  Recognizing time expressions is a fundamental ...      7\n",
       "8  When facing large-scale image datasets, online...      8\n",
       "9  Temporal modeling in videos is a fundamental y...      9"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contents = get_all_abstracts()\n",
    "print('共',len(contents),'篇論文\\n')\n",
    "\n",
    "documents = pd.DataFrame(data=contents,columns=['abstract'])\n",
    "documents['index'] = documents.index\n",
    "documents[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 預處理的全部論文摘要"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [consider, problem, actively, elicit, preferen...\n",
       "1    [investigate, task, distractor, generation, mu...\n",
       "2    [common, representation, formalism, plan, desc...\n",
       "3    [statistical, relational, learn, model, powerf...\n",
       "4    [multimodal, representation, learn, gain, deep...\n",
       "5    [reinforcement, learn, show, advantage, image,...\n",
       "6    [select, appropriate, tutor, help, action, acc...\n",
       "7    [recognize, time, expression, fundamental, imp...\n",
       "8    [face, large, scale, image, datasets, online, ...\n",
       "9    [temporal, model, video, fundamental, challeng...\n",
       "Name: abstract, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs = documents['abstract'].map(preprocess)\n",
    "processed_docs[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 產生字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "共 1558 個字\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dictionary = corpora.Dictionary.load('../corpus/dict_bigram_filtered.dict')\n",
    "print('共',len(dictionary),'個字\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 產生 bag of words corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "共 1343 筆\n"
     ]
    }
   ],
   "source": [
    "bow_corpus = corpora.MmCorpus('../corpus/corpus_bigram_filtered.mm')\n",
    "print('共',len(bow_corpus),'筆')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 產生TF-IDF corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "corpus_tfidf = tfidf[bow_corpus]\n",
    "\n",
    "print('共',len(corpus_tfidf),'筆')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load LDA models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = '10'\n",
    "num_words = 10\n",
    "\n",
    "file_name = '../models/lda_bigram_bow_filtered_topic_' + num_topics + '.model'\n",
    "lda_model = models.ldamodel.LdaModel.load(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### show topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0\n",
      "Words: 0.013*\"generate\" + 0.012*\"knowledge\" + 0.010*\"generation\" + 0.009*\"answer\" + 0.009*\"relation\" + 0.008*\"entity\" + 0.007*\"human\" + 0.007*\"dataset\" + 0.007*\"program\" + 0.007*\"question\"\n",
      "\n",
      "Topic: 1\n",
      "Words: 0.018*\"translation\" + 0.018*\"sequence\" + 0.015*\"recurrent\" + 0.014*\"memory\" + 0.013*\"student\" + 0.010*\"machine\" + 0.010*\"recurrent_neural\" + 0.008*\"inference\" + 0.008*\"resource\" + 0.008*\"temporal\"\n",
      "\n",
      "Topic: 2\n",
      "Words: 0.038*\"graph\" + 0.024*\"structure\" + 0.021*\"embed\" + 0.019*\"object\" + 0.017*\"detection\" + 0.010*\"nod\" + 0.010*\"attribute\" + 0.010*\"relation\" + 0.007*\"representation\" + 0.007*\"world\"\n",
      "\n",
      "Topic: 3\n",
      "Words: 0.067*\"label\" + 0.025*\"class\" + 0.023*\"classification\" + 0.014*\"multi\" + 0.010*\"hash\" + 0.010*\"supervise\" + 0.009*\"classifier\" + 0.009*\"source\" + 0.008*\"distribution\" + 0.008*\"instance\"\n",
      "\n",
      "Topic: 4\n",
      "Words: 0.035*\"domain\" + 0.020*\"video\" + 0.019*\"target\" + 0.016*\"temporal\" + 0.016*\"transfer\" + 0.012*\"source\" + 0.012*\"spatial\" + 0.010*\"event\" + 0.010*\"action\" + 0.010*\"representation\"\n",
      "\n",
      "Topic: 5\n",
      "Words: 0.021*\"word\" + 0.017*\"user\" + 0.017*\"text\" + 0.016*\"language\" + 0.016*\"sentence\" + 0.012*\"attention\" + 0.011*\"context\" + 0.010*\"representation\" + 0.010*\"question\" + 0.009*\"semantic\"\n",
      "\n",
      "Topic: 6\n",
      "Words: 0.033*\"agent\" + 0.015*\"decision\" + 0.014*\"game\" + 0.013*\"policy\" + 0.011*\"reinforcement\" + 0.011*\"action\" + 0.009*\"reinforcement_learn\" + 0.008*\"study\" + 0.007*\"search\" + 0.007*\"reward\"\n",
      "\n",
      "Topic: 7\n",
      "Words: 0.014*\"sample\" + 0.013*\"function\" + 0.012*\"optimization\" + 0.009*\"cluster\" + 0.008*\"matrix\" + 0.008*\"number\" + 0.007*\"gradient\" + 0.006*\"high\" + 0.006*\"approximate\" + 0.006*\"space\"\n",
      "\n",
      "Topic: 8\n",
      "Words: 0.019*\"plan\" + 0.011*\"prediction\" + 0.010*\"attack\" + 0.008*\"temporal\" + 0.008*\"adversarial\" + 0.007*\"traffic\" + 0.007*\"frame\" + 0.007*\"solution\" + 0.007*\"solve\" + 0.006*\"heuristic\"\n",
      "\n",
      "Topic: 9\n",
      "Words: 0.057*\"image\" + 0.021*\"deep\" + 0.020*\"view\" + 0.015*\"layer\" + 0.015*\"multi\" + 0.013*\"convolutional\" + 0.011*\"representation\" + 0.011*\"shape\" + 0.010*\"level\" + 0.008*\"object\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model.print_topics(): #num_topics=3, num_words=3\n",
    "    print('Topic: {}\\nWords: {}\\n'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can only concatenate str (not \"int\") to str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-69306b2fb156>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#columns = ['1','2','3','4','5']\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_topics\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: can only concatenate str (not \"int\") to str"
     ]
    }
   ],
   "source": [
    "K = num_topics\n",
    "\n",
    "topicWordProbMat = lda_model.print_topics(num_words=num_words)\n",
    "\n",
    "#columns = ['1','2','3','4','5']\n",
    "columns = range(1,num_topics+1)\n",
    "\n",
    "df = pd.DataFrame(columns = columns)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "# 40 will be resized later to match number of words in DC\n",
    "zz = np.zeros(shape=(80,K))\n",
    "\n",
    "last_number = 0\n",
    "DC = {}\n",
    "\n",
    "for x in range (num_words): #取每個topic前10個字\n",
    "    data= pd.DataFrame(columns=columns,index=[0])\n",
    "    for i in range(num_topics):\n",
    "        data[columns[i]] = \"\"\n",
    "    df = df.append(data,ignore_index=True)  \n",
    "    \n",
    "for line in topicWordProbMat:\n",
    "    topic_id,words = line #一個line是一個topic\n",
    "    probs = words.split(\"+\")\n",
    "    y = 0 #用來算第幾個word\n",
    "    for pr in probs:    \n",
    "        a = pr.split(\"*\")\n",
    "        df.iloc[y,topic_id] = a[1] #該word\n",
    "       \n",
    "        if a[1] in DC:\n",
    "            zz[DC[a[1]]][topic_id] = a[0] #該word的機率\n",
    "        else:\n",
    "            zz[last_number][topic_id] = a[0]\n",
    "            DC[a[1]] = last_number\n",
    "            last_number = last_number+1\n",
    "        y = y + 1\n",
    "\n",
    "print(df)\n",
    "print('\\n')\n",
    "\n",
    "print(DC)\n",
    "print('字典字數：',len(DC))\n",
    "print('\\n')\n",
    "\n",
    "print(zz)\n",
    "print(zz.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "zz = np.resize(zz,(len(DC.keys()),zz.shape[1]))\n",
    "\n",
    "for val, key in enumerate(DC.keys()):\n",
    "        plt.text(-3.5, val + 0.1, key,\n",
    "                 horizontalalignment='right',\n",
    "                 verticalalignment='center'\n",
    "                 )\n",
    "\n",
    "#plt.figure(figsize=(10,50))\n",
    "plt.imshow(zz, cmap='hot', interpolation='nearest',aspect=0.4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "若主題數制定過少，則會有歌詞不相近的歌詞卻被分到同一個主題底下的情形；\n",
    "反之，若主題數制定過多，則會有歌詞相近的歌詞被分到不同主題底下。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = range(1,5+1)\n",
    "data = pd.DataFrame({columns[0]:\"\",\n",
    "                     columns[1]:\"\",\n",
    "                     columns[2]:\"\",\n",
    "                     columns[3]:\"\",\n",
    "                     columns[4]:\"\",\n",
    "                    },index=[0])\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = range(1,5+1)\n",
    "data = pd.DataFrame(columns=columns,index=[0])\n",
    "for i in range(len(columns)):\n",
    "    data[columns[i]] = \"\"\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
