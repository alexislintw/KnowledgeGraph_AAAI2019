{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 比較 LDA topic numbers\n",
    "\n",
    "參考資料\n",
    "- https://blog.csdn.net/sinat_26917383/article/details/79357700\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-02 16:42:29,496 : DEBUG : Loaded backend module://ipykernel.pylab.backend_inline version unknown.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from enum import Enum\n",
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from gensim import corpora, models\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\n",
    "                    level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自定義 data types and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContentType(Enum):\n",
    "    TIT = 'title'\n",
    "    ABS = 'abstract'\n",
    "    AUT = 'author'\n",
    "    SEC = 'section'\n",
    "    \n",
    "def get_contents(content_type):\n",
    "    all_contents = []\n",
    "    dataset_path = '../dataset'\n",
    "    for file in os.listdir(dataset_path):\n",
    "        file_path = os.path.join(dataset_path, file)\n",
    "        if os.path.isfile(file_path):\n",
    "            with open(file_path) as f:  \n",
    "                line = f.readlines()\n",
    "                if content_type == ContentType.AUT:\n",
    "                    line = line[1]\n",
    "                elif content_type == ContentType.SEC:\n",
    "                    line = line[2]\n",
    "                elif content_type == ContentType.ABS:\n",
    "                    line = line[3]\n",
    "                else:\n",
    "                    line = line[0]\n",
    "                line = line.strip()\n",
    "                all_contents.append(line)\n",
    "        else:\n",
    "            print(file_path + ' does not exist.')\n",
    "    return all_contents\n",
    "\n",
    "\n",
    "def get_all_titles():\n",
    "    return get_contents(ContentType.TIT)\n",
    "\n",
    "def get_all_authors():        \n",
    "    return get_contents(ContentType.AUT)\n",
    "\n",
    "def get_all_sections():\n",
    "    return get_contents(ContentType.SEC)\n",
    "\n",
    "def get_all_abstracts():\n",
    "    return get_contents(ContentType.ABS)\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            token = wordnet_lemmatizer.lemmatize(token, pos='v')\n",
    "            token = wordnet_lemmatizer.lemmatize(token, pos='n')\n",
    "            result.append(token)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 取出所有摘要"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "共 1343 篇論文\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>We consider the problem of actively eliciting ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>We investigate the task of distractor generati...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The most common representation formalisms for ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Statistical relational learning models are pow...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Multimodal representation learning is gaining ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Reinforcement learning (RL) has shown its adva...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Selecting appropriate tutoring help actions th...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Recognizing time expressions is a fundamental ...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>When facing large-scale image datasets, online...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Temporal modeling in videos is a fundamental y...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            abstract  index\n",
       "0  We consider the problem of actively eliciting ...      0\n",
       "1  We investigate the task of distractor generati...      1\n",
       "2  The most common representation formalisms for ...      2\n",
       "3  Statistical relational learning models are pow...      3\n",
       "4  Multimodal representation learning is gaining ...      4\n",
       "5  Reinforcement learning (RL) has shown its adva...      5\n",
       "6  Selecting appropriate tutoring help actions th...      6\n",
       "7  Recognizing time expressions is a fundamental ...      7\n",
       "8  When facing large-scale image datasets, online...      8\n",
       "9  Temporal modeling in videos is a fundamental y...      9"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contents = get_all_abstracts()\n",
    "print('共',len(contents),'篇論文\\n')\n",
    "\n",
    "documents = pd.DataFrame(data=contents,columns=['abstract'])\n",
    "documents['index'] = documents.index\n",
    "documents[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 預處理的全部論文摘要"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [consider, problem, actively, elicit, preferen...\n",
       "1    [investigate, task, distractor, generation, mu...\n",
       "2    [common, representation, formalism, plan, desc...\n",
       "3    [statistical, relational, learn, model, powerf...\n",
       "4    [multimodal, representation, learn, gain, deep...\n",
       "5    [reinforcement, learn, show, advantage, image,...\n",
       "6    [select, appropriate, tutor, help, action, acc...\n",
       "7    [recognize, time, expression, fundamental, imp...\n",
       "8    [face, large, scale, image, datasets, online, ...\n",
       "9    [temporal, model, video, fundamental, challeng...\n",
       "Name: abstract, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs = documents['abstract'].map(preprocess)\n",
    "processed_docs[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "### 產生字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-02 16:42:37,504 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-11-02 16:42:37,614 : INFO : built Dictionary(6927 unique tokens: ['active', 'actively', 'adaptive', 'aggregation', 'algorithm']...) from 1343 documents (total 121739 corpus positions)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "共 6927 個字\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "print('共',len(dictionary),'個字\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 產生 bag of words corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "共 1343 筆\n"
     ]
    }
   ],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "\n",
    "print('共',len(bow_corpus),'筆')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 產生 TF-IDF Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-02 16:42:39,917 : INFO : collecting document frequencies\n",
      "2019-11-02 16:42:39,920 : INFO : PROGRESS: processing document #0\n",
      "2019-11-02 16:42:39,947 : INFO : calculating IDF weights for 1343 documents and 6926 features (84803 matrix non-zeros)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "共 1343 筆\n"
     ]
    }
   ],
   "source": [
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "corpus_tfidf = tfidf[bow_corpus]\n",
    "\n",
    "print('共',len(corpus_tfidf),'筆')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train LDA models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running LDA using Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lda_model = gensim.models.LdaMulticore(bow_corpus, \n",
    "#                                       num_topics=num_topics, \n",
    "#                                       id2word=dictionary, \n",
    "#                                       passes=150,\n",
    "#                                       workers=2,\n",
    "#                                       eval_every=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running LDA using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-02 16:42:44,202 : INFO : using symmetric alpha at 0.2\n",
      "2019-11-02 16:42:44,203 : INFO : using symmetric eta at 0.2\n",
      "2019-11-02 16:42:44,209 : INFO : using serial LDA version on this node\n",
      "2019-11-02 16:42:44,222 : INFO : running online LDA training, 5 topics, 20 passes over the supplied corpus of 1343 documents, updating every 6000 documents, evaluating every ~1343 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "2019-11-02 16:42:44,224 : INFO : training LDA model using 3 processes\n",
      "2019-11-02 16:42:44,261 : DEBUG : worker process entering E-step loop\n",
      "2019-11-02 16:42:44,276 : DEBUG : getting a new job\n",
      "2019-11-02 16:42:44,275 : DEBUG : worker process entering E-step loop\n",
      "2019-11-02 16:42:44,285 : DEBUG : getting a new job\n",
      "2019-11-02 16:42:44,285 : DEBUG : worker process entering E-step loop\n",
      "2019-11-02 16:42:44,292 : DEBUG : getting a new job\n",
      "2019-11-02 16:42:44,693 : INFO : PROGRESS: pass 0, dispatched chunk #0 = documents up to #1343/1343, outstanding queue size 1\n",
      "2019-11-02 16:42:45,023 : DEBUG : processing chunk #0 of 1343 documents\n",
      "2019-11-02 16:42:45,025 : DEBUG : performing inference on a chunk of 1343 documents\n",
      "2019-11-02 16:42:45,626 : DEBUG : 1249/1343 documents converged within 50 iterations\n",
      "2019-11-02 16:42:45,639 : DEBUG : processed chunk, queuing the result\n",
      "2019-11-02 16:42:45,644 : DEBUG : result put\n",
      "2019-11-02 16:42:45,648 : DEBUG : updating topics\n",
      "2019-11-02 16:42:45,649 : DEBUG : getting a new job\n",
      "2019-11-02 16:42:45,657 : INFO : topic #0 (0.200): 0.002*\"image\" + 0.002*\"plan\" + 0.002*\"model\" + 0.002*\"feature\" + 0.002*\"agent\" + 0.002*\"problem\" + 0.002*\"network\" + 0.001*\"domain\" + 0.001*\"learn\" + 0.001*\"content\"\n",
      "2019-11-02 16:42:45,661 : INFO : topic #1 (0.200): 0.002*\"model\" + 0.002*\"label\" + 0.002*\"learn\" + 0.002*\"network\" + 0.002*\"data\" + 0.002*\"domain\" + 0.002*\"image\" + 0.002*\"method\" + 0.002*\"video\" + 0.002*\"feature\"\n",
      "2019-11-02 16:42:45,663 : INFO : topic #2 (0.200): 0.002*\"model\" + 0.002*\"network\" + 0.002*\"feature\" + 0.001*\"agent\" + 0.001*\"learn\" + 0.001*\"train\" + 0.001*\"multi\" + 0.001*\"approach\" + 0.001*\"method\" + 0.001*\"algorithm\"\n",
      "2019-11-02 16:42:45,666 : INFO : topic #3 (0.200): 0.002*\"attribute\" + 0.002*\"network\" + 0.002*\"data\" + 0.002*\"graph\" + 0.002*\"model\" + 0.002*\"method\" + 0.002*\"learn\" + 0.001*\"feature\" + 0.001*\"class\" + 0.001*\"task\"\n",
      "2019-11-02 16:42:45,669 : INFO : topic #4 (0.200): 0.002*\"agent\" + 0.002*\"task\" + 0.002*\"model\" + 0.002*\"learn\" + 0.002*\"algorithm\" + 0.002*\"word\" + 0.002*\"feature\" + 0.002*\"data\" + 0.002*\"problem\" + 0.001*\"network\"\n",
      "2019-11-02 16:42:45,670 : INFO : topic diff=2.654860, rho=1.000000\n",
      "2019-11-02 16:42:45,697 : DEBUG : bound: at document #0\n",
      "2019-11-02 16:42:46,412 : INFO : -10.042 per-word bound, 1054.2 perplexity estimate based on a held-out corpus of 1343 documents with 7958 words\n",
      "2019-11-02 16:42:46,796 : INFO : PROGRESS: pass 1, dispatched chunk #0 = documents up to #1343/1343, outstanding queue size 1\n",
      "2019-11-02 16:42:47,127 : DEBUG : processing chunk #0 of 1343 documents\n",
      "2019-11-02 16:42:47,129 : DEBUG : performing inference on a chunk of 1343 documents\n",
      "2019-11-02 16:42:47,498 : DEBUG : 1291/1343 documents converged within 50 iterations\n",
      "2019-11-02 16:42:47,511 : DEBUG : processed chunk, queuing the result\n",
      "2019-11-02 16:42:47,513 : DEBUG : result put\n",
      "2019-11-02 16:42:47,516 : DEBUG : updating topics\n",
      "2019-11-02 16:42:47,517 : DEBUG : getting a new job\n",
      "2019-11-02 16:42:47,523 : INFO : topic #0 (0.200): 0.002*\"image\" + 0.002*\"plan\" + 0.002*\"model\" + 0.002*\"agent\" + 0.001*\"problem\" + 0.001*\"content\" + 0.001*\"algorithm\" + 0.001*\"learn\" + 0.001*\"network\" + 0.001*\"train\"\n",
      "2019-11-02 16:42:47,525 : INFO : topic #1 (0.200): 0.002*\"label\" + 0.002*\"model\" + 0.002*\"learn\" + 0.002*\"domain\" + 0.002*\"image\" + 0.002*\"data\" + 0.002*\"network\" + 0.002*\"video\" + 0.002*\"method\" + 0.002*\"feature\"\n",
      "2019-11-02 16:42:47,526 : INFO : topic #2 (0.200): 0.002*\"model\" + 0.002*\"network\" + 0.001*\"feature\" + 0.001*\"train\" + 0.001*\"multi\" + 0.001*\"learn\" + 0.001*\"agent\" + 0.001*\"approach\" + 0.001*\"algorithm\" + 0.001*\"method\"\n",
      "2019-11-02 16:42:47,528 : INFO : topic #3 (0.200): 0.002*\"attribute\" + 0.002*\"graph\" + 0.002*\"network\" + 0.002*\"model\" + 0.002*\"data\" + 0.002*\"method\" + 0.001*\"feature\" + 0.001*\"learn\" + 0.001*\"class\" + 0.001*\"action\"\n",
      "2019-11-02 16:42:47,530 : INFO : topic #4 (0.200): 0.003*\"agent\" + 0.002*\"task\" + 0.002*\"model\" + 0.002*\"learn\" + 0.002*\"algorithm\" + 0.002*\"word\" + 0.002*\"data\" + 0.002*\"feature\" + 0.002*\"problem\" + 0.001*\"network\"\n",
      "2019-11-02 16:42:47,533 : INFO : topic diff=0.065411, rho=0.611818\n",
      "2019-11-02 16:42:47,558 : DEBUG : bound: at document #0\n",
      "2019-11-02 16:42:48,312 : INFO : -10.020 per-word bound, 1038.6 perplexity estimate based on a held-out corpus of 1343 documents with 7958 words\n",
      "2019-11-02 16:42:48,710 : INFO : PROGRESS: pass 2, dispatched chunk #0 = documents up to #1343/1343, outstanding queue size 1\n",
      "2019-11-02 16:42:49,040 : DEBUG : processing chunk #0 of 1343 documents\n",
      "2019-11-02 16:42:49,041 : DEBUG : performing inference on a chunk of 1343 documents\n",
      "2019-11-02 16:42:49,444 : DEBUG : 1279/1343 documents converged within 50 iterations\n",
      "2019-11-02 16:42:49,467 : DEBUG : processed chunk, queuing the result\n",
      "2019-11-02 16:42:49,481 : DEBUG : updating topics\n",
      "2019-11-02 16:42:49,479 : DEBUG : result put\n",
      "2019-11-02 16:42:49,493 : INFO : topic #0 (0.200): 0.002*\"plan\" + 0.002*\"agent\" + 0.002*\"image\" + 0.001*\"model\" + 0.001*\"problem\" + 0.001*\"content\" + 0.001*\"algorithm\" + 0.001*\"approach\" + 0.001*\"knowledge\" + 0.001*\"learn\"\n",
      "2019-11-02 16:42:49,483 : DEBUG : getting a new job\n",
      "2019-11-02 16:42:49,498 : INFO : topic #1 (0.200): 0.003*\"label\" + 0.003*\"model\" + 0.003*\"image\" + 0.002*\"learn\" + 0.002*\"feature\" + 0.002*\"domain\" + 0.002*\"data\" + 0.002*\"network\" + 0.002*\"video\" + 0.002*\"method\"\n",
      "2019-11-02 16:42:49,503 : INFO : topic #2 (0.200): 0.002*\"network\" + 0.001*\"model\" + 0.001*\"agent\" + 0.001*\"multi\" + 0.001*\"learn\" + 0.001*\"algorithm\" + 0.001*\"answer\" + 0.001*\"approach\" + 0.001*\"community\" + 0.001*\"feature\"\n",
      "2019-11-02 16:42:49,505 : INFO : topic #3 (0.200): 0.002*\"attribute\" + 0.002*\"graph\" + 0.002*\"network\" + 0.002*\"model\" + 0.001*\"data\" + 0.001*\"method\" + 0.001*\"user\" + 0.001*\"feature\" + 0.001*\"learn\" + 0.001*\"action\"\n",
      "2019-11-02 16:42:49,513 : INFO : topic #4 (0.200): 0.003*\"agent\" + 0.002*\"task\" + 0.002*\"model\" + 0.002*\"learn\" + 0.002*\"algorithm\" + 0.002*\"problem\" + 0.002*\"word\" + 0.002*\"data\" + 0.001*\"feature\" + 0.001*\"user\"\n",
      "2019-11-02 16:42:49,515 : INFO : topic diff=0.083458, rho=0.521889\n",
      "2019-11-02 16:42:49,553 : DEBUG : bound: at document #0\n",
      "2019-11-02 16:42:50,296 : INFO : -9.982 per-word bound, 1011.0 perplexity estimate based on a held-out corpus of 1343 documents with 7958 words\n",
      "2019-11-02 16:42:50,676 : INFO : PROGRESS: pass 3, dispatched chunk #0 = documents up to #1343/1343, outstanding queue size 1\n",
      "2019-11-02 16:42:50,993 : DEBUG : processing chunk #0 of 1343 documents\n",
      "2019-11-02 16:42:50,994 : DEBUG : performing inference on a chunk of 1343 documents\n",
      "2019-11-02 16:42:51,373 : DEBUG : 1289/1343 documents converged within 50 iterations\n",
      "2019-11-02 16:42:51,387 : DEBUG : processed chunk, queuing the result\n",
      "2019-11-02 16:42:51,389 : DEBUG : result put\n",
      "2019-11-02 16:42:51,392 : DEBUG : updating topics\n",
      "2019-11-02 16:42:51,398 : INFO : topic #0 (0.200): 0.002*\"plan\" + 0.002*\"agent\" + 0.001*\"problem\" + 0.001*\"content\" + 0.001*\"algorithm\" + 0.001*\"model\" + 0.001*\"image\" + 0.001*\"knowledge\" + 0.001*\"approach\" + 0.001*\"document\"\n",
      "2019-11-02 16:42:51,394 : DEBUG : getting a new job\n",
      "2019-11-02 16:42:51,403 : INFO : topic #1 (0.200): 0.003*\"label\" + 0.003*\"image\" + 0.003*\"model\" + 0.003*\"feature\" + 0.003*\"learn\" + 0.003*\"network\" + 0.003*\"data\" + 0.003*\"domain\" + 0.002*\"method\" + 0.002*\"train\"\n",
      "2019-11-02 16:42:51,406 : INFO : topic #2 (0.200): 0.001*\"network\" + 0.001*\"model\" + 0.001*\"agent\" + 0.001*\"community\" + 0.001*\"multi\" + 0.001*\"algorithm\" + 0.001*\"query\" + 0.001*\"answer\" + 0.001*\"learn\" + 0.001*\"approach\"\n",
      "2019-11-02 16:42:51,409 : INFO : topic #3 (0.200): 0.002*\"attribute\" + 0.002*\"graph\" + 0.001*\"network\" + 0.001*\"user\" + 0.001*\"model\" + 0.001*\"data\" + 0.001*\"decision\" + 0.001*\"algorithm\" + 0.001*\"action\" + 0.001*\"method\"\n",
      "2019-11-02 16:42:51,413 : INFO : topic #4 (0.200): 0.003*\"agent\" + 0.002*\"algorithm\" + 0.002*\"task\" + 0.002*\"model\" + 0.002*\"learn\" + 0.002*\"problem\" + 0.002*\"user\" + 0.001*\"approach\" + 0.001*\"question\" + 0.001*\"word\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-02 16:42:51,415 : INFO : topic diff=0.108032, rho=0.462671\n",
      "2019-11-02 16:42:51,447 : DEBUG : bound: at document #0\n",
      "2019-11-02 16:42:52,153 : INFO : -9.917 per-word bound, 966.4 perplexity estimate based on a held-out corpus of 1343 documents with 7958 words\n",
      "2019-11-02 16:42:52,546 : INFO : PROGRESS: pass 4, dispatched chunk #0 = documents up to #1343/1343, outstanding queue size 1\n",
      "2019-11-02 16:42:52,858 : DEBUG : processing chunk #0 of 1343 documents\n",
      "2019-11-02 16:42:52,860 : DEBUG : performing inference on a chunk of 1343 documents\n",
      "2019-11-02 16:42:53,215 : DEBUG : 1312/1343 documents converged within 50 iterations\n",
      "2019-11-02 16:42:53,226 : DEBUG : processed chunk, queuing the result\n",
      "2019-11-02 16:42:53,229 : DEBUG : result put\n",
      "2019-11-02 16:42:53,232 : DEBUG : updating topics\n",
      "2019-11-02 16:42:53,233 : DEBUG : getting a new job\n",
      "2019-11-02 16:42:53,238 : INFO : topic #0 (0.200): 0.002*\"plan\" + 0.002*\"agent\" + 0.001*\"problem\" + 0.001*\"algorithm\" + 0.001*\"content\" + 0.001*\"document\" + 0.001*\"approach\" + 0.001*\"knowledge\" + 0.001*\"robot\" + 0.001*\"model\"\n",
      "2019-11-02 16:42:53,239 : INFO : topic #1 (0.200): 0.003*\"label\" + 0.003*\"model\" + 0.003*\"image\" + 0.003*\"feature\" + 0.003*\"learn\" + 0.003*\"network\" + 0.003*\"data\" + 0.003*\"method\" + 0.003*\"train\" + 0.002*\"domain\"\n",
      "2019-11-02 16:42:53,241 : INFO : topic #2 (0.200): 0.001*\"network\" + 0.001*\"agent\" + 0.001*\"community\" + 0.001*\"query\" + 0.001*\"model\" + 0.001*\"algorithm\" + 0.001*\"multi\" + 0.001*\"answer\" + 0.001*\"number\" + 0.001*\"rule\"\n",
      "2019-11-02 16:42:53,242 : INFO : topic #3 (0.200): 0.002*\"attribute\" + 0.001*\"graph\" + 0.001*\"user\" + 0.001*\"network\" + 0.001*\"data\" + 0.001*\"algorithm\" + 0.001*\"decision\" + 0.001*\"model\" + 0.001*\"time\" + 0.001*\"problem\"\n",
      "2019-11-02 16:42:53,244 : INFO : topic #4 (0.200): 0.003*\"agent\" + 0.002*\"algorithm\" + 0.002*\"task\" + 0.002*\"learn\" + 0.002*\"model\" + 0.002*\"problem\" + 0.001*\"user\" + 0.001*\"question\" + 0.001*\"approach\" + 0.001*\"time\"\n",
      "2019-11-02 16:42:53,247 : INFO : topic diff=0.124652, rho=0.419905\n",
      "2019-11-02 16:42:53,266 : DEBUG : bound: at document #0\n",
      "2019-11-02 16:42:53,953 : INFO : -9.836 per-word bound, 914.1 perplexity estimate based on a held-out corpus of 1343 documents with 7958 words\n",
      "2019-11-02 16:42:54,327 : INFO : PROGRESS: pass 5, dispatched chunk #0 = documents up to #1343/1343, outstanding queue size 1\n",
      "2019-11-02 16:42:54,647 : DEBUG : processing chunk #0 of 1343 documents\n",
      "2019-11-02 16:42:54,649 : DEBUG : performing inference on a chunk of 1343 documents\n",
      "2019-11-02 16:42:54,960 : DEBUG : 1318/1343 documents converged within 50 iterations\n",
      "2019-11-02 16:42:54,969 : DEBUG : processed chunk, queuing the result\n",
      "2019-11-02 16:42:54,973 : DEBUG : result put\n",
      "2019-11-02 16:42:54,976 : DEBUG : updating topics\n",
      "2019-11-02 16:42:54,978 : DEBUG : getting a new job\n",
      "2019-11-02 16:42:54,984 : INFO : topic #0 (0.200): 0.002*\"agent\" + 0.002*\"plan\" + 0.001*\"problem\" + 0.001*\"document\" + 0.001*\"algorithm\" + 0.001*\"robot\" + 0.001*\"content\" + 0.001*\"dictionary\" + 0.001*\"vote\" + 0.001*\"game\"\n",
      "2019-11-02 16:42:54,985 : INFO : topic #1 (0.200): 0.003*\"model\" + 0.003*\"label\" + 0.003*\"feature\" + 0.003*\"image\" + 0.003*\"network\" + 0.003*\"learn\" + 0.003*\"data\" + 0.003*\"method\" + 0.003*\"train\" + 0.002*\"domain\"\n",
      "2019-11-02 16:42:54,987 : INFO : topic #2 (0.200): 0.001*\"agent\" + 0.001*\"community\" + 0.001*\"network\" + 0.001*\"query\" + 0.001*\"event\" + 0.001*\"argument\" + 0.001*\"rule\" + 0.001*\"algorithm\" + 0.001*\"snns\" + 0.001*\"multi\"\n",
      "2019-11-02 16:42:54,990 : INFO : topic #3 (0.200): 0.001*\"attribute\" + 0.001*\"graph\" + 0.001*\"user\" + 0.001*\"search\" + 0.001*\"solver\" + 0.001*\"decision\" + 0.001*\"algorithm\" + 0.001*\"time\" + 0.001*\"network\" + 0.001*\"problem\"\n",
      "2019-11-02 16:42:54,991 : INFO : topic #4 (0.200): 0.003*\"agent\" + 0.002*\"algorithm\" + 0.002*\"problem\" + 0.002*\"task\" + 0.001*\"learn\" + 0.001*\"user\" + 0.001*\"model\" + 0.001*\"game\" + 0.001*\"search\" + 0.001*\"question\"\n",
      "2019-11-02 16:42:54,992 : INFO : topic diff=0.126736, rho=0.387158\n",
      "2019-11-02 16:42:55,012 : DEBUG : bound: at document #0\n",
      "2019-11-02 16:42:55,660 : INFO : -9.757 per-word bound, 865.2 perplexity estimate based on a held-out corpus of 1343 documents with 7958 words\n",
      "2019-11-02 16:42:56,031 : INFO : PROGRESS: pass 6, dispatched chunk #0 = documents up to #1343/1343, outstanding queue size 1\n",
      "2019-11-02 16:42:56,343 : DEBUG : processing chunk #0 of 1343 documents\n",
      "2019-11-02 16:42:56,344 : DEBUG : performing inference on a chunk of 1343 documents\n",
      "2019-11-02 16:42:56,639 : DEBUG : 1327/1343 documents converged within 50 iterations\n",
      "2019-11-02 16:42:56,653 : DEBUG : processed chunk, queuing the result\n",
      "2019-11-02 16:42:56,656 : DEBUG : result put\n",
      "2019-11-02 16:42:56,658 : DEBUG : updating topics\n",
      "2019-11-02 16:42:56,659 : DEBUG : getting a new job\n",
      "2019-11-02 16:42:56,664 : INFO : topic #0 (0.200): 0.002*\"agent\" + 0.002*\"plan\" + 0.001*\"robot\" + 0.001*\"document\" + 0.001*\"dictionary\" + 0.001*\"vote\" + 0.001*\"algorithm\" + 0.001*\"game\" + 0.001*\"problem\" + 0.001*\"reward\"\n",
      "2019-11-02 16:42:56,666 : INFO : topic #1 (0.200): 0.003*\"model\" + 0.003*\"network\" + 0.003*\"feature\" + 0.003*\"learn\" + 0.003*\"image\" + 0.003*\"label\" + 0.003*\"data\" + 0.003*\"method\" + 0.003*\"train\" + 0.002*\"task\"\n",
      "2019-11-02 16:42:56,668 : INFO : topic #2 (0.200): 0.001*\"agent\" + 0.001*\"event\" + 0.001*\"community\" + 0.001*\"argument\" + 0.001*\"query\" + 0.001*\"snns\" + 0.001*\"ontology\" + 0.001*\"rule\" + 0.001*\"distortion\" + 0.001*\"expansion\"\n",
      "2019-11-02 16:42:56,669 : INFO : topic #3 (0.200): 0.001*\"attribute\" + 0.001*\"graph\" + 0.001*\"solver\" + 0.001*\"satisfiability\" + 0.001*\"search\" + 0.001*\"user\" + 0.001*\"decision\" + 0.001*\"algorithm\" + 0.001*\"time\" + 0.001*\"game\"\n",
      "2019-11-02 16:42:56,671 : INFO : topic #4 (0.200): 0.004*\"agent\" + 0.002*\"algorithm\" + 0.001*\"problem\" + 0.001*\"game\" + 0.001*\"search\" + 0.001*\"user\" + 0.001*\"reason\" + 0.001*\"task\" + 0.001*\"time\" + 0.001*\"bound\"\n",
      "2019-11-02 16:42:56,674 : INFO : topic diff=0.121463, rho=0.361044\n",
      "2019-11-02 16:42:56,694 : DEBUG : bound: at document #0\n",
      "2019-11-02 16:42:57,308 : INFO : -9.686 per-word bound, 823.4 perplexity estimate based on a held-out corpus of 1343 documents with 7958 words\n",
      "2019-11-02 16:42:57,701 : INFO : PROGRESS: pass 7, dispatched chunk #0 = documents up to #1343/1343, outstanding queue size 1\n",
      "2019-11-02 16:42:58,020 : DEBUG : processing chunk #0 of 1343 documents\n",
      "2019-11-02 16:42:58,022 : DEBUG : performing inference on a chunk of 1343 documents\n",
      "2019-11-02 16:42:58,291 : DEBUG : 1331/1343 documents converged within 50 iterations\n",
      "2019-11-02 16:42:58,304 : DEBUG : processed chunk, queuing the result\n",
      "2019-11-02 16:42:58,312 : DEBUG : result put\n",
      "2019-11-02 16:42:58,314 : DEBUG : updating topics\n",
      "2019-11-02 16:42:58,315 : DEBUG : getting a new job\n",
      "2019-11-02 16:42:58,320 : INFO : topic #0 (0.200): 0.002*\"agent\" + 0.001*\"plan\" + 0.001*\"dictionary\" + 0.001*\"vote\" + 0.001*\"robot\" + 0.001*\"document\" + 0.001*\"event\" + 0.001*\"valuation\" + 0.001*\"swap\" + 0.001*\"game\"\n",
      "2019-11-02 16:42:58,321 : INFO : topic #1 (0.200): 0.003*\"model\" + 0.003*\"network\" + 0.003*\"learn\" + 0.003*\"feature\" + 0.003*\"image\" + 0.003*\"data\" + 0.003*\"label\" + 0.003*\"method\" + 0.003*\"train\" + 0.002*\"task\"\n",
      "2019-11-02 16:42:58,324 : INFO : topic #2 (0.200): 0.001*\"snns\" + 0.001*\"event\" + 0.001*\"agent\" + 0.001*\"argument\" + 0.001*\"ontology\" + 0.001*\"stance\" + 0.001*\"distortion\" + 0.001*\"expansion\" + 0.001*\"comment\" + 0.001*\"community\"\n",
      "2019-11-02 16:42:58,326 : INFO : topic #3 (0.200): 0.001*\"satisfiability\" + 0.001*\"solver\" + 0.001*\"attribute\" + 0.001*\"graph\" + 0.001*\"search\" + 0.001*\"user\" + 0.001*\"explanation\" + 0.001*\"rule\" + 0.001*\"decision\" + 0.001*\"algorithm\"\n",
      "2019-11-02 16:42:58,328 : INFO : topic #4 (0.200): 0.004*\"agent\" + 0.002*\"algorithm\" + 0.001*\"game\" + 0.001*\"problem\" + 0.001*\"search\" + 0.001*\"reason\" + 0.001*\"user\" + 0.001*\"document\" + 0.001*\"bound\" + 0.001*\"goal\"\n",
      "2019-11-02 16:42:58,331 : INFO : topic diff=0.112547, rho=0.339588\n",
      "2019-11-02 16:42:58,359 : DEBUG : bound: at document #0\n",
      "2019-11-02 16:42:58,988 : INFO : -9.624 per-word bound, 789.2 perplexity estimate based on a held-out corpus of 1343 documents with 7958 words\n",
      "2019-11-02 16:42:59,366 : INFO : PROGRESS: pass 8, dispatched chunk #0 = documents up to #1343/1343, outstanding queue size 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-02 16:42:59,687 : DEBUG : processing chunk #0 of 1343 documents\n",
      "2019-11-02 16:42:59,689 : DEBUG : performing inference on a chunk of 1343 documents\n",
      "2019-11-02 16:42:59,922 : DEBUG : 1337/1343 documents converged within 50 iterations\n",
      "2019-11-02 16:42:59,934 : DEBUG : processed chunk, queuing the result\n",
      "2019-11-02 16:42:59,940 : DEBUG : updating topics\n",
      "2019-11-02 16:42:59,939 : DEBUG : result put\n",
      "2019-11-02 16:42:59,942 : DEBUG : getting a new job\n",
      "2019-11-02 16:42:59,946 : INFO : topic #0 (0.200): 0.001*\"agent\" + 0.001*\"plan\" + 0.001*\"dictionary\" + 0.001*\"vote\" + 0.001*\"robot\" + 0.001*\"valuation\" + 0.001*\"swap\" + 0.001*\"document\" + 0.001*\"division\" + 0.001*\"event\"\n",
      "2019-11-02 16:42:59,947 : INFO : topic #1 (0.200): 0.003*\"model\" + 0.003*\"learn\" + 0.003*\"network\" + 0.003*\"feature\" + 0.003*\"data\" + 0.003*\"image\" + 0.003*\"label\" + 0.003*\"method\" + 0.002*\"train\" + 0.002*\"task\"\n",
      "2019-11-02 16:42:59,950 : INFO : topic #2 (0.200): 0.001*\"snns\" + 0.001*\"stance\" + 0.001*\"ontology\" + 0.001*\"argument\" + 0.001*\"comment\" + 0.001*\"poverty\" + 0.001*\"distortion\" + 0.001*\"agent\" + 0.001*\"event\" + 0.001*\"expansion\"\n",
      "2019-11-02 16:42:59,952 : INFO : topic #3 (0.200): 0.001*\"satisfiability\" + 0.001*\"solver\" + 0.001*\"attribute\" + 0.001*\"graph\" + 0.001*\"session\" + 0.001*\"search\" + 0.001*\"profile\" + 0.001*\"explanation\" + 0.001*\"rule\" + 0.001*\"gaze\"\n",
      "2019-11-02 16:42:59,953 : INFO : topic #4 (0.200): 0.004*\"agent\" + 0.002*\"algorithm\" + 0.001*\"game\" + 0.001*\"search\" + 0.001*\"problem\" + 0.001*\"reason\" + 0.001*\"preference\" + 0.001*\"document\" + 0.001*\"goal\" + 0.001*\"user\"\n",
      "2019-11-02 16:42:59,954 : INFO : topic diff=0.102199, rho=0.321553\n",
      "2019-11-02 16:42:59,972 : DEBUG : bound: at document #0\n",
      "2019-11-02 16:43:00,571 : INFO : -9.573 per-word bound, 761.5 perplexity estimate based on a held-out corpus of 1343 documents with 7958 words\n",
      "2019-11-02 16:43:00,949 : INFO : PROGRESS: pass 9, dispatched chunk #0 = documents up to #1343/1343, outstanding queue size 1\n",
      "2019-11-02 16:43:01,257 : DEBUG : processing chunk #0 of 1343 documents\n",
      "2019-11-02 16:43:01,259 : DEBUG : performing inference on a chunk of 1343 documents\n",
      "2019-11-02 16:43:01,497 : DEBUG : 1337/1343 documents converged within 50 iterations\n",
      "2019-11-02 16:43:01,509 : DEBUG : processed chunk, queuing the result\n",
      "2019-11-02 16:43:01,512 : DEBUG : result put\n",
      "2019-11-02 16:43:01,515 : DEBUG : updating topics\n",
      "2019-11-02 16:43:01,515 : DEBUG : getting a new job\n",
      "2019-11-02 16:43:01,521 : INFO : topic #0 (0.200): 0.001*\"agent\" + 0.001*\"dictionary\" + 0.001*\"vote\" + 0.001*\"plan\" + 0.001*\"robot\" + 0.001*\"valuation\" + 0.001*\"swap\" + 0.001*\"division\" + 0.001*\"auction\" + 0.001*\"fair\"\n",
      "2019-11-02 16:43:01,521 : INFO : topic #1 (0.200): 0.003*\"model\" + 0.003*\"learn\" + 0.003*\"network\" + 0.003*\"feature\" + 0.003*\"data\" + 0.003*\"image\" + 0.003*\"label\" + 0.003*\"method\" + 0.002*\"train\" + 0.002*\"task\"\n",
      "2019-11-02 16:43:01,523 : INFO : topic #2 (0.200): 0.001*\"snns\" + 0.001*\"stance\" + 0.001*\"ontology\" + 0.001*\"comment\" + 0.001*\"poverty\" + 0.001*\"argument\" + 0.001*\"distortion\" + 0.001*\"fisher\" + 0.001*\"strategic\" + 0.001*\"discount\"\n",
      "2019-11-02 16:43:01,524 : INFO : topic #3 (0.200): 0.001*\"satisfiability\" + 0.001*\"solver\" + 0.001*\"session\" + 0.001*\"gaze\" + 0.001*\"profile\" + 0.001*\"voter\" + 0.001*\"compilation\" + 0.001*\"survival\" + 0.001*\"truthful\" + 0.001*\"explanation\"\n",
      "2019-11-02 16:43:01,526 : INFO : topic #4 (0.200): 0.004*\"agent\" + 0.001*\"algorithm\" + 0.001*\"game\" + 0.001*\"preference\" + 0.001*\"search\" + 0.001*\"reason\" + 0.001*\"goal\" + 0.001*\"problem\" + 0.001*\"mapf\" + 0.001*\"user\"\n",
      "2019-11-02 16:43:01,527 : INFO : topic diff=0.091377, rho=0.306117\n",
      "2019-11-02 16:43:01,546 : DEBUG : bound: at document #0\n",
      "2019-11-02 16:43:02,119 : INFO : -9.530 per-word bound, 739.3 perplexity estimate based on a held-out corpus of 1343 documents with 7958 words\n",
      "2019-11-02 16:43:02,499 : INFO : PROGRESS: pass 10, dispatched chunk #0 = documents up to #1343/1343, outstanding queue size 1\n",
      "2019-11-02 16:43:02,804 : DEBUG : processing chunk #0 of 1343 documents\n",
      "2019-11-02 16:43:02,806 : DEBUG : performing inference on a chunk of 1343 documents\n",
      "2019-11-02 16:43:03,018 : DEBUG : 1338/1343 documents converged within 50 iterations\n",
      "2019-11-02 16:43:03,031 : DEBUG : processed chunk, queuing the result\n",
      "2019-11-02 16:43:03,034 : DEBUG : result put\n",
      "2019-11-02 16:43:03,036 : DEBUG : updating topics\n",
      "2019-11-02 16:43:03,036 : DEBUG : getting a new job\n",
      "2019-11-02 16:43:03,042 : INFO : topic #0 (0.200): 0.001*\"dictionary\" + 0.001*\"vote\" + 0.001*\"agent\" + 0.001*\"valuation\" + 0.001*\"division\" + 0.001*\"swap\" + 0.001*\"robot\" + 0.001*\"plan\" + 0.001*\"auction\" + 0.001*\"fair\"\n",
      "2019-11-02 16:43:03,043 : INFO : topic #1 (0.200): 0.003*\"model\" + 0.003*\"learn\" + 0.003*\"network\" + 0.003*\"feature\" + 0.003*\"data\" + 0.003*\"image\" + 0.003*\"label\" + 0.003*\"method\" + 0.002*\"task\" + 0.002*\"train\"\n",
      "2019-11-02 16:43:03,045 : INFO : topic #2 (0.200): 0.001*\"snns\" + 0.001*\"stance\" + 0.001*\"poverty\" + 0.001*\"comment\" + 0.001*\"ontology\" + 0.001*\"fisher\" + 0.001*\"strategic\" + 0.001*\"centrality\" + 0.001*\"idiom\" + 0.001*\"discount\"\n",
      "2019-11-02 16:43:03,046 : INFO : topic #3 (0.200): 0.001*\"satisfiability\" + 0.001*\"solver\" + 0.001*\"session\" + 0.001*\"gaze\" + 0.001*\"voter\" + 0.001*\"compilation\" + 0.001*\"survival\" + 0.001*\"truthful\" + 0.001*\"metrical\" + 0.001*\"vote\"\n",
      "2019-11-02 16:43:03,047 : INFO : topic #4 (0.200): 0.003*\"agent\" + 0.001*\"preference\" + 0.001*\"game\" + 0.001*\"algorithm\" + 0.001*\"mapf\" + 0.001*\"search\" + 0.001*\"teach\" + 0.001*\"reason\" + 0.001*\"goal\" + 0.001*\"execution\"\n",
      "2019-11-02 16:43:03,048 : INFO : topic diff=0.081459, rho=0.292709\n",
      "2019-11-02 16:43:03,067 : DEBUG : bound: at document #0\n",
      "2019-11-02 16:43:03,645 : INFO : -9.495 per-word bound, 721.7 perplexity estimate based on a held-out corpus of 1343 documents with 7958 words\n",
      "2019-11-02 16:43:04,020 : INFO : PROGRESS: pass 11, dispatched chunk #0 = documents up to #1343/1343, outstanding queue size 1\n",
      "2019-11-02 16:43:04,323 : DEBUG : processing chunk #0 of 1343 documents\n",
      "2019-11-02 16:43:04,325 : DEBUG : performing inference on a chunk of 1343 documents\n",
      "2019-11-02 16:43:04,543 : DEBUG : 1337/1343 documents converged within 50 iterations\n",
      "2019-11-02 16:43:04,556 : DEBUG : processed chunk, queuing the result\n",
      "2019-11-02 16:43:04,560 : DEBUG : result put\n",
      "2019-11-02 16:43:04,562 : DEBUG : updating topics\n",
      "2019-11-02 16:43:04,562 : DEBUG : getting a new job\n",
      "2019-11-02 16:43:04,568 : INFO : topic #0 (0.200): 0.001*\"dictionary\" + 0.001*\"vote\" + 0.001*\"agent\" + 0.001*\"valuation\" + 0.001*\"division\" + 0.001*\"swap\" + 0.001*\"auction\" + 0.001*\"robot\" + 0.001*\"plan\" + 0.001*\"fair\"\n",
      "2019-11-02 16:43:04,569 : INFO : topic #1 (0.200): 0.003*\"model\" + 0.003*\"learn\" + 0.003*\"network\" + 0.003*\"feature\" + 0.003*\"data\" + 0.003*\"image\" + 0.003*\"label\" + 0.003*\"method\" + 0.002*\"task\" + 0.002*\"train\"\n",
      "2019-11-02 16:43:04,570 : INFO : topic #2 (0.200): 0.001*\"snns\" + 0.001*\"stance\" + 0.001*\"poverty\" + 0.001*\"comment\" + 0.001*\"ontology\" + 0.001*\"fisher\" + 0.001*\"strategic\" + 0.001*\"centrality\" + 0.001*\"idiom\" + 0.001*\"board\"\n",
      "2019-11-02 16:43:04,572 : INFO : topic #3 (0.200): 0.001*\"satisfiability\" + 0.001*\"session\" + 0.001*\"solver\" + 0.001*\"gaze\" + 0.001*\"voter\" + 0.001*\"compilation\" + 0.001*\"survival\" + 0.001*\"truthful\" + 0.001*\"metrical\" + 0.001*\"vote\"\n",
      "2019-11-02 16:43:04,574 : INFO : topic #4 (0.200): 0.003*\"agent\" + 0.001*\"mapf\" + 0.001*\"preference\" + 0.001*\"game\" + 0.001*\"teach\" + 0.001*\"puzzle\" + 0.001*\"algorithm\" + 0.001*\"execution\" + 0.001*\"conversation\" + 0.001*\"equivalence\"\n",
      "2019-11-02 16:43:04,575 : INFO : topic diff=0.071584, rho=0.280922\n",
      "2019-11-02 16:43:04,594 : DEBUG : bound: at document #0\n",
      "2019-11-02 16:43:05,147 : INFO : -9.467 per-word bound, 707.6 perplexity estimate based on a held-out corpus of 1343 documents with 7958 words\n",
      "2019-11-02 16:43:05,525 : INFO : PROGRESS: pass 12, dispatched chunk #0 = documents up to #1343/1343, outstanding queue size 1\n",
      "2019-11-02 16:43:05,829 : DEBUG : processing chunk #0 of 1343 documents\n",
      "2019-11-02 16:43:05,831 : DEBUG : performing inference on a chunk of 1343 documents\n",
      "2019-11-02 16:43:06,044 : DEBUG : 1340/1343 documents converged within 50 iterations\n",
      "2019-11-02 16:43:06,060 : DEBUG : processed chunk, queuing the result\n",
      "2019-11-02 16:43:06,063 : DEBUG : updating topics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-02 16:43:06,062 : DEBUG : result put\n",
      "2019-11-02 16:43:06,064 : DEBUG : getting a new job\n",
      "2019-11-02 16:43:06,068 : INFO : topic #0 (0.200): 0.001*\"dictionary\" + 0.001*\"vote\" + 0.001*\"valuation\" + 0.001*\"division\" + 0.001*\"swap\" + 0.001*\"agent\" + 0.001*\"auction\" + 0.001*\"subtasks\" + 0.001*\"robot\" + 0.001*\"fair\"\n",
      "2019-11-02 16:43:06,074 : INFO : topic #1 (0.200): 0.003*\"model\" + 0.003*\"learn\" + 0.003*\"network\" + 0.003*\"feature\" + 0.003*\"data\" + 0.003*\"image\" + 0.003*\"label\" + 0.003*\"method\" + 0.002*\"task\" + 0.002*\"train\"\n",
      "2019-11-02 16:43:06,076 : INFO : topic #2 (0.200): 0.001*\"snns\" + 0.001*\"stance\" + 0.001*\"poverty\" + 0.001*\"comment\" + 0.001*\"fisher\" + 0.001*\"strategic\" + 0.001*\"centrality\" + 0.001*\"idiom\" + 0.001*\"ontology\" + 0.001*\"board\"\n",
      "2019-11-02 16:43:06,080 : INFO : topic #3 (0.200): 0.001*\"satisfiability\" + 0.001*\"session\" + 0.001*\"gaze\" + 0.001*\"solver\" + 0.001*\"voter\" + 0.001*\"truthful\" + 0.001*\"survival\" + 0.001*\"compilation\" + 0.001*\"metrical\" + 0.001*\"circuit\"\n",
      "2019-11-02 16:43:06,082 : INFO : topic #4 (0.200): 0.003*\"agent\" + 0.001*\"mapf\" + 0.001*\"puzzle\" + 0.001*\"preference\" + 0.001*\"equivalence\" + 0.001*\"execution\" + 0.001*\"conversation\" + 0.001*\"game\" + 0.001*\"teach\" + 0.001*\"mcmc\"\n",
      "2019-11-02 16:43:06,084 : INFO : topic diff=0.062824, rho=0.270453\n",
      "2019-11-02 16:43:06,101 : DEBUG : bound: at document #0\n",
      "2019-11-02 16:43:06,663 : INFO : -9.444 per-word bound, 696.3 perplexity estimate based on a held-out corpus of 1343 documents with 7958 words\n",
      "2019-11-02 16:43:07,049 : INFO : PROGRESS: pass 13, dispatched chunk #0 = documents up to #1343/1343, outstanding queue size 1\n",
      "2019-11-02 16:43:07,353 : DEBUG : processing chunk #0 of 1343 documents\n",
      "2019-11-02 16:43:07,354 : DEBUG : performing inference on a chunk of 1343 documents\n",
      "2019-11-02 16:43:07,543 : DEBUG : 1342/1343 documents converged within 50 iterations\n",
      "2019-11-02 16:43:07,555 : DEBUG : processed chunk, queuing the result\n",
      "2019-11-02 16:43:07,561 : DEBUG : updating topics\n",
      "2019-11-02 16:43:07,559 : DEBUG : result put\n",
      "2019-11-02 16:43:07,563 : DEBUG : getting a new job\n",
      "2019-11-02 16:43:07,567 : INFO : topic #0 (0.200): 0.001*\"dictionary\" + 0.001*\"vote\" + 0.001*\"valuation\" + 0.001*\"division\" + 0.001*\"swap\" + 0.001*\"auction\" + 0.001*\"subtasks\" + 0.001*\"agent\" + 0.001*\"indivisible\" + 0.001*\"multilingual\"\n",
      "2019-11-02 16:43:07,568 : INFO : topic #1 (0.200): 0.003*\"model\" + 0.003*\"learn\" + 0.003*\"network\" + 0.003*\"feature\" + 0.003*\"data\" + 0.003*\"image\" + 0.003*\"method\" + 0.002*\"label\" + 0.002*\"task\" + 0.002*\"algorithm\"\n",
      "2019-11-02 16:43:07,572 : INFO : topic #2 (0.200): 0.001*\"snns\" + 0.001*\"stance\" + 0.001*\"poverty\" + 0.001*\"comment\" + 0.001*\"fisher\" + 0.001*\"idiom\" + 0.001*\"centrality\" + 0.001*\"board\" + 0.001*\"grasp\" + 0.001*\"blame\"\n",
      "2019-11-02 16:43:07,574 : INFO : topic #3 (0.200): 0.001*\"satisfiability\" + 0.001*\"session\" + 0.001*\"gaze\" + 0.001*\"truthful\" + 0.001*\"voter\" + 0.001*\"survival\" + 0.001*\"solver\" + 0.001*\"metrical\" + 0.001*\"circuit\" + 0.001*\"compilation\"\n",
      "2019-11-02 16:43:07,576 : INFO : topic #4 (0.200): 0.003*\"agent\" + 0.001*\"mapf\" + 0.001*\"puzzle\" + 0.001*\"equivalence\" + 0.001*\"conversation\" + 0.001*\"execution\" + 0.001*\"mcmc\" + 0.001*\"reader\" + 0.001*\"preference\" + 0.001*\"teach\"\n",
      "2019-11-02 16:43:07,579 : INFO : topic diff=0.054794, rho=0.261073\n",
      "2019-11-02 16:43:07,599 : DEBUG : bound: at document #0\n",
      "2019-11-02 16:43:08,125 : INFO : -9.425 per-word bound, 687.2 perplexity estimate based on a held-out corpus of 1343 documents with 7958 words\n",
      "2019-11-02 16:43:08,502 : INFO : PROGRESS: pass 14, dispatched chunk #0 = documents up to #1343/1343, outstanding queue size 1\n",
      "2019-11-02 16:43:08,815 : DEBUG : processing chunk #0 of 1343 documents\n",
      "2019-11-02 16:43:08,816 : DEBUG : performing inference on a chunk of 1343 documents\n",
      "2019-11-02 16:43:08,997 : DEBUG : 1343/1343 documents converged within 50 iterations\n",
      "2019-11-02 16:43:09,012 : DEBUG : processed chunk, queuing the result\n",
      "2019-11-02 16:43:09,014 : DEBUG : result put\n",
      "2019-11-02 16:43:09,015 : DEBUG : updating topics\n",
      "2019-11-02 16:43:09,015 : DEBUG : getting a new job\n",
      "2019-11-02 16:43:09,020 : INFO : topic #0 (0.200): 0.001*\"dictionary\" + 0.001*\"valuation\" + 0.001*\"vote\" + 0.001*\"division\" + 0.001*\"swap\" + 0.001*\"auction\" + 0.001*\"subtasks\" + 0.001*\"indivisible\" + 0.001*\"multilingual\" + 0.001*\"boaf\"\n",
      "2019-11-02 16:43:09,021 : INFO : topic #1 (0.200): 0.003*\"model\" + 0.003*\"learn\" + 0.003*\"network\" + 0.003*\"feature\" + 0.003*\"data\" + 0.003*\"image\" + 0.002*\"method\" + 0.002*\"label\" + 0.002*\"algorithm\" + 0.002*\"task\"\n",
      "2019-11-02 16:43:09,024 : INFO : topic #2 (0.200): 0.001*\"snns\" + 0.001*\"stance\" + 0.001*\"poverty\" + 0.001*\"comment\" + 0.001*\"fisher\" + 0.001*\"idiom\" + 0.001*\"centrality\" + 0.001*\"board\" + 0.001*\"blame\" + 0.001*\"equation\"\n",
      "2019-11-02 16:43:09,025 : INFO : topic #3 (0.200): 0.001*\"satisfiability\" + 0.001*\"session\" + 0.001*\"gaze\" + 0.001*\"truthful\" + 0.001*\"survival\" + 0.001*\"metrical\" + 0.001*\"voter\" + 0.001*\"circuit\" + 0.001*\"aesthetic\" + 0.001*\"nonverbal\"\n",
      "2019-11-02 16:43:09,027 : INFO : topic #4 (0.200): 0.002*\"agent\" + 0.001*\"mapf\" + 0.001*\"puzzle\" + 0.001*\"equivalence\" + 0.001*\"mcmc\" + 0.001*\"conversation\" + 0.001*\"reader\" + 0.001*\"execution\" + 0.001*\"teach\" + 0.001*\"preference\"\n",
      "2019-11-02 16:43:09,029 : INFO : topic diff=0.047192, rho=0.252607\n",
      "2019-11-02 16:43:09,048 : DEBUG : bound: at document #0\n",
      "2019-11-02 16:43:09,581 : INFO : -9.410 per-word bound, 680.2 perplexity estimate based on a held-out corpus of 1343 documents with 7958 words\n",
      "2019-11-02 16:43:09,951 : INFO : PROGRESS: pass 15, dispatched chunk #0 = documents up to #1343/1343, outstanding queue size 1\n",
      "2019-11-02 16:43:10,249 : DEBUG : processing chunk #0 of 1343 documents\n",
      "2019-11-02 16:43:10,251 : DEBUG : performing inference on a chunk of 1343 documents\n",
      "2019-11-02 16:43:10,434 : DEBUG : 1340/1343 documents converged within 50 iterations\n",
      "2019-11-02 16:43:10,446 : DEBUG : processed chunk, queuing the result\n",
      "2019-11-02 16:43:10,448 : DEBUG : result put\n",
      "2019-11-02 16:43:10,451 : DEBUG : updating topics\n",
      "2019-11-02 16:43:10,452 : DEBUG : getting a new job\n",
      "2019-11-02 16:43:10,458 : INFO : topic #0 (0.200): 0.001*\"dictionary\" + 0.001*\"valuation\" + 0.001*\"vote\" + 0.001*\"division\" + 0.001*\"auction\" + 0.001*\"swap\" + 0.001*\"subtasks\" + 0.001*\"indivisible\" + 0.001*\"multilingual\" + 0.001*\"boaf\"\n",
      "2019-11-02 16:43:10,459 : INFO : topic #1 (0.200): 0.003*\"model\" + 0.003*\"learn\" + 0.003*\"network\" + 0.003*\"feature\" + 0.003*\"data\" + 0.002*\"image\" + 0.002*\"method\" + 0.002*\"label\" + 0.002*\"algorithm\" + 0.002*\"task\"\n",
      "2019-11-02 16:43:10,461 : INFO : topic #2 (0.200): 0.001*\"snns\" + 0.001*\"stance\" + 0.001*\"poverty\" + 0.001*\"comment\" + 0.001*\"fisher\" + 0.001*\"idiom\" + 0.001*\"centrality\" + 0.001*\"blame\" + 0.001*\"board\" + 0.001*\"equation\"\n",
      "2019-11-02 16:43:10,463 : INFO : topic #3 (0.200): 0.001*\"satisfiability\" + 0.001*\"session\" + 0.001*\"gaze\" + 0.001*\"truthful\" + 0.001*\"metrical\" + 0.001*\"survival\" + 0.001*\"circuit\" + 0.001*\"aesthetic\" + 0.001*\"voter\" + 0.001*\"nonverbal\"\n",
      "2019-11-02 16:43:10,464 : INFO : topic #4 (0.200): 0.002*\"agent\" + 0.001*\"mapf\" + 0.001*\"puzzle\" + 0.001*\"equivalence\" + 0.001*\"mcmc\" + 0.001*\"conversation\" + 0.001*\"reader\" + 0.001*\"execution\" + 0.001*\"teach\" + 0.001*\"epistemic\"\n",
      "2019-11-02 16:43:10,465 : INFO : topic diff=0.040942, rho=0.244913\n",
      "2019-11-02 16:43:10,484 : DEBUG : bound: at document #0\n",
      "2019-11-02 16:43:11,012 : INFO : -9.398 per-word bound, 674.5 perplexity estimate based on a held-out corpus of 1343 documents with 7958 words\n",
      "2019-11-02 16:43:11,381 : INFO : PROGRESS: pass 16, dispatched chunk #0 = documents up to #1343/1343, outstanding queue size 1\n",
      "2019-11-02 16:43:11,693 : DEBUG : processing chunk #0 of 1343 documents\n",
      "2019-11-02 16:43:11,695 : DEBUG : performing inference on a chunk of 1343 documents\n",
      "2019-11-02 16:43:11,872 : DEBUG : 1343/1343 documents converged within 50 iterations\n",
      "2019-11-02 16:43:11,884 : DEBUG : processed chunk, queuing the result\n",
      "2019-11-02 16:43:11,889 : DEBUG : result put\n",
      "2019-11-02 16:43:11,893 : DEBUG : updating topics\n",
      "2019-11-02 16:43:11,894 : DEBUG : getting a new job\n",
      "2019-11-02 16:43:11,899 : INFO : topic #0 (0.200): 0.001*\"dictionary\" + 0.001*\"valuation\" + 0.001*\"vote\" + 0.001*\"auction\" + 0.001*\"division\" + 0.001*\"swap\" + 0.001*\"centrality\" + 0.001*\"indivisible\" + 0.001*\"multilingual\" + 0.001*\"boaf\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-02 16:43:11,901 : INFO : topic #1 (0.200): 0.003*\"model\" + 0.003*\"learn\" + 0.003*\"network\" + 0.003*\"feature\" + 0.003*\"data\" + 0.002*\"image\" + 0.002*\"method\" + 0.002*\"label\" + 0.002*\"algorithm\" + 0.002*\"task\"\n",
      "2019-11-02 16:43:11,904 : INFO : topic #2 (0.200): 0.001*\"snns\" + 0.001*\"stance\" + 0.001*\"poverty\" + 0.001*\"comment\" + 0.001*\"fisher\" + 0.001*\"idiom\" + 0.001*\"blame\" + 0.001*\"equation\" + 0.001*\"board\" + 0.001*\"seller\"\n",
      "2019-11-02 16:43:11,905 : INFO : topic #3 (0.200): 0.002*\"satisfiability\" + 0.001*\"session\" + 0.001*\"gaze\" + 0.001*\"truthful\" + 0.001*\"metrical\" + 0.001*\"survival\" + 0.001*\"circuit\" + 0.001*\"aesthetic\" + 0.001*\"nonverbal\" + 0.001*\"hyperbolic\"\n",
      "2019-11-02 16:43:11,908 : INFO : topic #4 (0.200): 0.002*\"agent\" + 0.001*\"mapf\" + 0.001*\"puzzle\" + 0.001*\"equivalence\" + 0.001*\"mcmc\" + 0.001*\"conversation\" + 0.001*\"reader\" + 0.001*\"epistemic\" + 0.001*\"execution\" + 0.001*\"wcsps\"\n",
      "2019-11-02 16:43:11,911 : INFO : topic diff=0.035602, rho=0.237883\n",
      "2019-11-02 16:43:11,929 : DEBUG : bound: at document #0\n",
      "2019-11-02 16:43:12,463 : INFO : -9.388 per-word bound, 669.8 perplexity estimate based on a held-out corpus of 1343 documents with 7958 words\n",
      "2019-11-02 16:43:12,843 : INFO : PROGRESS: pass 17, dispatched chunk #0 = documents up to #1343/1343, outstanding queue size 1\n",
      "2019-11-02 16:43:13,144 : DEBUG : processing chunk #0 of 1343 documents\n",
      "2019-11-02 16:43:13,145 : DEBUG : performing inference on a chunk of 1343 documents\n",
      "2019-11-02 16:43:13,321 : DEBUG : 1343/1343 documents converged within 50 iterations\n",
      "2019-11-02 16:43:13,334 : DEBUG : processed chunk, queuing the result\n",
      "2019-11-02 16:43:13,338 : DEBUG : result put\n",
      "2019-11-02 16:43:13,340 : DEBUG : updating topics\n",
      "2019-11-02 16:43:13,341 : DEBUG : getting a new job\n",
      "2019-11-02 16:43:13,347 : INFO : topic #0 (0.200): 0.001*\"dictionary\" + 0.001*\"valuation\" + 0.001*\"vote\" + 0.001*\"auction\" + 0.001*\"centrality\" + 0.001*\"division\" + 0.001*\"swap\" + 0.001*\"indivisible\" + 0.001*\"multilingual\" + 0.001*\"boaf\"\n",
      "2019-11-02 16:43:13,347 : INFO : topic #1 (0.200): 0.003*\"model\" + 0.003*\"learn\" + 0.003*\"network\" + 0.003*\"data\" + 0.003*\"feature\" + 0.002*\"image\" + 0.002*\"method\" + 0.002*\"label\" + 0.002*\"algorithm\" + 0.002*\"task\"\n",
      "2019-11-02 16:43:13,348 : INFO : topic #2 (0.200): 0.001*\"snns\" + 0.001*\"stance\" + 0.001*\"poverty\" + 0.001*\"idiom\" + 0.001*\"comment\" + 0.001*\"fisher\" + 0.001*\"blame\" + 0.001*\"equation\" + 0.001*\"board\" + 0.001*\"seller\"\n",
      "2019-11-02 16:43:13,350 : INFO : topic #3 (0.200): 0.002*\"satisfiability\" + 0.001*\"session\" + 0.001*\"gaze\" + 0.001*\"truthful\" + 0.001*\"metrical\" + 0.001*\"circuit\" + 0.001*\"aesthetic\" + 0.001*\"nonverbal\" + 0.001*\"hyperbolic\" + 0.001*\"survival\"\n",
      "2019-11-02 16:43:13,352 : INFO : topic #4 (0.200): 0.001*\"mapf\" + 0.001*\"agent\" + 0.001*\"puzzle\" + 0.001*\"equivalence\" + 0.001*\"mcmc\" + 0.001*\"conversation\" + 0.001*\"reader\" + 0.001*\"epistemic\" + 0.001*\"wcsps\" + 0.001*\"music\"\n",
      "2019-11-02 16:43:13,353 : INFO : topic diff=0.030849, rho=0.231425\n",
      "2019-11-02 16:43:13,370 : DEBUG : bound: at document #0\n",
      "2019-11-02 16:43:13,909 : INFO : -9.379 per-word bound, 666.0 perplexity estimate based on a held-out corpus of 1343 documents with 7958 words\n",
      "2019-11-02 16:43:14,286 : INFO : PROGRESS: pass 18, dispatched chunk #0 = documents up to #1343/1343, outstanding queue size 1\n",
      "2019-11-02 16:43:14,589 : DEBUG : processing chunk #0 of 1343 documents\n",
      "2019-11-02 16:43:14,590 : DEBUG : performing inference on a chunk of 1343 documents\n",
      "2019-11-02 16:43:14,759 : DEBUG : 1343/1343 documents converged within 50 iterations\n",
      "2019-11-02 16:43:14,770 : DEBUG : processed chunk, queuing the result\n",
      "2019-11-02 16:43:14,774 : DEBUG : updating topics\n",
      "2019-11-02 16:43:14,773 : DEBUG : result put\n",
      "2019-11-02 16:43:14,780 : INFO : topic #0 (0.200): 0.001*\"dictionary\" + 0.001*\"valuation\" + 0.001*\"centrality\" + 0.001*\"auction\" + 0.001*\"vote\" + 0.001*\"division\" + 0.001*\"indivisible\" + 0.001*\"multilingual\" + 0.001*\"swap\" + 0.001*\"boaf\"\n",
      "2019-11-02 16:43:14,782 : INFO : topic #1 (0.200): 0.003*\"model\" + 0.003*\"learn\" + 0.003*\"network\" + 0.003*\"data\" + 0.003*\"feature\" + 0.002*\"image\" + 0.002*\"method\" + 0.002*\"algorithm\" + 0.002*\"label\" + 0.002*\"task\"\n",
      "2019-11-02 16:43:14,778 : DEBUG : getting a new job\n",
      "2019-11-02 16:43:14,784 : INFO : topic #2 (0.200): 0.001*\"snns\" + 0.001*\"stance\" + 0.001*\"poverty\" + 0.001*\"idiom\" + 0.001*\"comment\" + 0.001*\"blame\" + 0.001*\"equation\" + 0.001*\"fisher\" + 0.001*\"board\" + 0.001*\"seller\"\n",
      "2019-11-02 16:43:14,786 : INFO : topic #3 (0.200): 0.002*\"satisfiability\" + 0.001*\"session\" + 0.001*\"gaze\" + 0.001*\"truthful\" + 0.001*\"metrical\" + 0.001*\"circuit\" + 0.001*\"aesthetic\" + 0.001*\"hyperbolic\" + 0.001*\"nonverbal\" + 0.001*\"pamc\"\n",
      "2019-11-02 16:43:14,787 : INFO : topic #4 (0.200): 0.001*\"mapf\" + 0.001*\"puzzle\" + 0.001*\"equivalence\" + 0.001*\"agent\" + 0.001*\"mcmc\" + 0.001*\"conversation\" + 0.001*\"epistemic\" + 0.001*\"wcsps\" + 0.001*\"music\" + 0.001*\"reader\"\n",
      "2019-11-02 16:43:14,788 : INFO : topic diff=0.026808, rho=0.225466\n",
      "2019-11-02 16:43:14,804 : DEBUG : bound: at document #0\n",
      "2019-11-02 16:43:15,323 : INFO : -9.373 per-word bound, 662.9 perplexity estimate based on a held-out corpus of 1343 documents with 7958 words\n",
      "2019-11-02 16:43:15,705 : INFO : PROGRESS: pass 19, dispatched chunk #0 = documents up to #1343/1343, outstanding queue size 1\n",
      "2019-11-02 16:43:16,006 : DEBUG : processing chunk #0 of 1343 documents\n",
      "2019-11-02 16:43:16,008 : DEBUG : performing inference on a chunk of 1343 documents\n",
      "2019-11-02 16:43:16,180 : DEBUG : 1343/1343 documents converged within 50 iterations\n",
      "2019-11-02 16:43:16,192 : DEBUG : processed chunk, queuing the result\n",
      "2019-11-02 16:43:16,196 : DEBUG : result put\n",
      "2019-11-02 16:43:16,197 : DEBUG : updating topics\n",
      "2019-11-02 16:43:16,199 : DEBUG : getting a new job\n",
      "2019-11-02 16:43:16,203 : INFO : topic #0 (0.200): 0.001*\"dictionary\" + 0.001*\"valuation\" + 0.001*\"centrality\" + 0.001*\"auction\" + 0.001*\"division\" + 0.001*\"vote\" + 0.001*\"multilingual\" + 0.001*\"boaf\" + 0.001*\"indivisible\" + 0.001*\"discussion\"\n",
      "2019-11-02 16:43:16,204 : INFO : topic #1 (0.200): 0.003*\"model\" + 0.003*\"learn\" + 0.003*\"network\" + 0.003*\"data\" + 0.003*\"feature\" + 0.002*\"image\" + 0.002*\"method\" + 0.002*\"algorithm\" + 0.002*\"label\" + 0.002*\"task\"\n",
      "2019-11-02 16:43:16,205 : INFO : topic #2 (0.200): 0.001*\"snns\" + 0.001*\"stance\" + 0.001*\"poverty\" + 0.001*\"idiom\" + 0.001*\"comment\" + 0.001*\"blame\" + 0.001*\"equation\" + 0.001*\"board\" + 0.001*\"blameworthiness\" + 0.001*\"seller\"\n",
      "2019-11-02 16:43:16,207 : INFO : topic #3 (0.200): 0.002*\"satisfiability\" + 0.001*\"session\" + 0.001*\"gaze\" + 0.001*\"truthful\" + 0.001*\"metrical\" + 0.001*\"circuit\" + 0.001*\"aesthetic\" + 0.001*\"hyperbolic\" + 0.001*\"nonverbal\" + 0.001*\"pamc\"\n",
      "2019-11-02 16:43:16,208 : INFO : topic #4 (0.200): 0.002*\"mapf\" + 0.001*\"puzzle\" + 0.001*\"equivalence\" + 0.001*\"mcmc\" + 0.001*\"agent\" + 0.001*\"epistemic\" + 0.001*\"wcsps\" + 0.001*\"music\" + 0.001*\"conversation\" + 0.001*\"negotiation\"\n",
      "2019-11-02 16:43:16,211 : INFO : topic diff=0.023385, rho=0.219945\n",
      "2019-11-02 16:43:16,229 : DEBUG : bound: at document #0\n",
      "2019-11-02 16:43:16,768 : INFO : -9.367 per-word bound, 660.4 perplexity estimate based on a held-out corpus of 1343 documents with 7958 words\n"
     ]
    }
   ],
   "source": [
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, \n",
    "                                       num_topics=num_topics, \n",
    "                                       id2word=dictionary, \n",
    "                                       passes=20,\n",
    "                                       eval_every=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 結論\n",
    "- topic num: 5\n",
    "- bow: 100次尚未全部收斂\n",
    "- tfidf: 第14次已全部收斂\n",
    "- \n",
    "- topic num: 10\n",
    "- bow: 100次尚未全部收斂\n",
    "- tfidf: 第11次已全部收斂\n",
    "- \n",
    "- topic num: 15\n",
    "- bow: 100次尚未全部收斂\n",
    "- tfidf: 第7次已全部收斂\n",
    "- \n",
    "- topic num: 20\n",
    "- bow: 100次尚未全部收斂\n",
    "- tfidf: 第3次已全部收斂\n",
    "-  \n",
    "- topic num: 25\n",
    "- tfidf: 第1次已全部收斂\n",
    "- \n",
    "- topic num: 30\n",
    "- tfidf: 第1次已全部收斂\n",
    "\n",
    "##### Topic num愈多，收斂愈快"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
