{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 以LDA Topics分佈 計算文件相似度\n",
    "\n",
    "### 參考資料\n",
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pyldavis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from enum import Enum\n",
    "from pprint import pprint\n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from gensim import corpora, models\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "from gensim.models.ldamodel import LdaModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自定義 data types and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContentType(Enum):\n",
    "    TIT = 'title'\n",
    "    ABS = 'abstract'\n",
    "    AUT = 'author'\n",
    "    SEC = 'section'\n",
    "    \n",
    "def get_contents(content_type):\n",
    "    all_contents = []\n",
    "    dataset_path = '../dataset'\n",
    "    for file in os.listdir(dataset_path):\n",
    "        file_path = os.path.join(dataset_path, file)\n",
    "        if os.path.isfile(file_path):\n",
    "            with open(file_path) as f:  \n",
    "                line = f.readlines()\n",
    "                if content_type == ContentType.AUT:\n",
    "                    line = line[1]\n",
    "                elif content_type == ContentType.SEC:\n",
    "                    line = line[2]\n",
    "                elif content_type == ContentType.ABS:\n",
    "                    line = line[3]\n",
    "                else:\n",
    "                    line = line[0]\n",
    "                line = line.strip()\n",
    "                all_contents.append(line)\n",
    "        else:\n",
    "            print(file_path + ' does not exist.')\n",
    "    return all_contents\n",
    "\n",
    "\n",
    "def get_all_titles():\n",
    "    return get_contents(ContentType.TIT)\n",
    "\n",
    "def get_all_authors():        \n",
    "    return get_contents(ContentType.AUT)\n",
    "\n",
    "def get_all_sections():\n",
    "    return get_contents(ContentType.SEC)\n",
    "\n",
    "def get_all_abstracts():\n",
    "    return get_contents(ContentType.ABS)\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            token = wordnet_lemmatizer.lemmatize(token, pos='v')\n",
    "            token = wordnet_lemmatizer.lemmatize(token, pos='n')\n",
    "            result.append(token)\n",
    "    return result\n",
    "\n",
    "def show_doc(head,doc_id):\n",
    "    print('[',head,':',doc_id,']\\n')\n",
    "    print(titles[doc_id],'\\n')\n",
    "    print(sections[doc_id],'\\n')\n",
    "    print(contents[doc_id],'\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 取出所有摘要"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "共 1343 篇論文\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>We consider the problem of actively eliciting ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>We investigate the task of distractor generati...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The most common representation formalisms for ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Statistical relational learning models are pow...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Multimodal representation learning is gaining ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Reinforcement learning (RL) has shown its adva...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Selecting appropriate tutoring help actions th...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Recognizing time expressions is a fundamental ...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>When facing large-scale image datasets, online...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Temporal modeling in videos is a fundamental y...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            abstract  index\n",
       "0  We consider the problem of actively eliciting ...      0\n",
       "1  We investigate the task of distractor generati...      1\n",
       "2  The most common representation formalisms for ...      2\n",
       "3  Statistical relational learning models are pow...      3\n",
       "4  Multimodal representation learning is gaining ...      4\n",
       "5  Reinforcement learning (RL) has shown its adva...      5\n",
       "6  Selecting appropriate tutoring help actions th...      6\n",
       "7  Recognizing time expressions is a fundamental ...      7\n",
       "8  When facing large-scale image datasets, online...      8\n",
       "9  Temporal modeling in videos is a fundamental y...      9"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contents = get_all_abstracts()\n",
    "print('共',len(contents),'篇論文\\n')\n",
    "\n",
    "documents = pd.DataFrame(data=contents,columns=['abstract'])\n",
    "documents['index'] = documents.index\n",
    "documents[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 預處理的全部論文摘要"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [consider, problem, actively, elicit, preferen...\n",
       "1    [investigate, task, distractor, generation, mu...\n",
       "2    [common, representation, formalism, plan, desc...\n",
       "3    [statistical, relational, learn, model, powerf...\n",
       "4    [multimodal, representation, learn, gain, deep...\n",
       "5    [reinforcement, learn, show, advantage, image,...\n",
       "6    [select, appropriate, tutor, help, action, acc...\n",
       "7    [recognize, time, expression, fundamental, imp...\n",
       "8    [face, large, scale, image, datasets, online, ...\n",
       "9    [temporal, model, video, fundamental, challeng...\n",
       "Name: abstract, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs = documents['abstract'].map(preprocess)\n",
    "processed_docs[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 產生字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "共 6927 個字\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "print('共',len(dictionary),'個字\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 產生 bag of words corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "共 1343 筆\n"
     ]
    }
   ],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "print('共',len(bow_corpus),'筆')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 產生TF-IDF Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "共 1343 筆\n"
     ]
    }
   ],
   "source": [
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "corpus_tfidf = tfidf[bow_corpus]\n",
    "print('共',len(corpus_tfidf),'筆')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load LDA models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexis/anaconda3/lib/python3.7/site-packages/smart_open/smart_open_lib.py:160: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  args, varargs, keywords, defaults = inspect.getargspec(kallable)\n",
      "/Users/alexis/anaconda3/lib/python3.7/site-packages/smart_open/smart_open_lib.py:160: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  args, varargs, keywords, defaults = inspect.getargspec(kallable)\n",
      "/Users/alexis/anaconda3/lib/python3.7/site-packages/smart_open/smart_open_lib.py:160: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  args, varargs, keywords, defaults = inspect.getargspec(kallable)\n"
     ]
    }
   ],
   "source": [
    "num_topics = '30'\n",
    "file_name = '../models/lda_tfidf_topic_' + num_topics + '.model'\n",
    "lda_model = models.ldamodel.LdaModel.load(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 找出最相似的文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = get_all_titles()\n",
    "sections = get_all_sections()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_doc_id = 137"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lda_mode' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-154-ea487abd2545>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpus_tfidf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msample_doc_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#lda_vec1 = lda_model[dd]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mlda_vec1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtup\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlda_mode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_document_topics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlda_vec1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lda_mode' is not defined"
     ]
    }
   ],
   "source": [
    "cos_sim = []\n",
    "dd = corpus_tfidf[sample_doc_id]\n",
    "#lda_vec1 = lda_model[dd]\n",
    "lda_vec1 = np.array([tup[1] for tup in lda_mode.get_document_topics(bow=dd)])\n",
    "print(lda_vec1)\n",
    "\n",
    "for i in range(len(corpus_tfidf)):\n",
    "    lda_vec2 = lda_model[corpus_tfidf[i]]\n",
    "    sim = gensim.matutils.cossim(lda_vec1, lda_vec2)\n",
    "    cos_sim.append(sim)\n",
    "\n",
    "cos_sim_doc_id = sorted(range(len(cos_sim)), key=lambda i: cos_sim[i])[-2:]\n",
    "cos_sim_doc_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hellinger distance\n",
    "- is useful for similarity between probability distributions (such as LDA topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(15, 0.055832896), (27, 0.75282466)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "869"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hd_sim = []\n",
    "lda_vec1 = lda_model[corpus_tfidf[sample_doc_id]]\n",
    "print(lda_vec1)\n",
    "for i in range(len(corpus_tfidf)):\n",
    "    lda_vec2 = lda_model[corpus_tfidf[i]]\n",
    "    dense1 = gensim.matutils.sparse2full(lda_vec1, lda_model.num_topics)\n",
    "    dense2 = gensim.matutils.sparse2full(lda_vec2, lda_model.num_topics)\n",
    "    sim = np.sqrt(0.5 * ((np.sqrt(dense1) - np.sqrt(dense2))**2).sum())\n",
    "    hd_sim.append(sim)\n",
    "\n",
    "hd_sim_doc_id = sorted(range(len(docs_sim)), key=lambda i: docs_sim[i])[1]\n",
    "hd_sim_doc_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic id: 27\n",
      "\n",
      "Score: 0.7513797879219055\t \n",
      "Topic: 0.006*\"model\" + 0.005*\"learn\" + 0.005*\"network\" + 0.005*\"data\" + 0.005*\"feature\" + 0.004*\"method\" + 0.004*\"image\" + 0.004*\"label\" + 0.004*\"task\" + 0.004*\"agent\" + 0.004*\"algorithm\" + 0.004*\"train\" + 0.004*\"problem\" + 0.004*\"approach\" + 0.004*\"domain\" + 0.004*\"information\" + 0.003*\"representation\" + 0.003*\"neural\" + 0.003*\"base\" + 0.003*\"word\" + 0.003*\"time\" + 0.003*\"graph\" + 0.003*\"multi\" + 0.003*\"text\" + 0.003*\"performance\" + 0.003*\"knowledge\" + 0.003*\"deep\" + 0.003*\"sample\" + 0.003*\"video\" + 0.003*\"structure\"\n",
      "\n",
      "Topic id: 15\n",
      "\n",
      "Score: 0.05764486640691757\t \n",
      "Topic: 0.003*\"customer\" + 0.003*\"dialogue\" + 0.003*\"check\" + 0.002*\"character\" + 0.002*\"macro\" + 0.002*\"provider\" + 0.002*\"dispersal\" + 0.002*\"acoustic\" + 0.002*\"gold\" + 0.002*\"removal\" + 0.002*\"speech\" + 0.002*\"attack\" + 0.002*\"forbid\" + 0.002*\"worker\" + 0.002*\"daml\" + 0.002*\"tamp\" + 0.002*\"peer\" + 0.002*\"reid\" + 0.002*\"satisfiability\" + 0.002*\"clip\" + 0.002*\"market\" + 0.001*\"disentangle\" + 0.001*\"equal\" + 0.001*\"valuable\" + 0.001*\"proxy\" + 0.001*\"experience\" + 0.001*\"vanilla\" + 0.001*\"semeval\" + 0.001*\"turn\" + 0.001*\"classically\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model[corpus_tfidf[sample_doc_id]], key=lambda tup: -1*tup[1]):\n",
    "    print('Topic id:',index)\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\\n\".format(score, lda_model.print_topic(index, 30)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic id: 13\n",
      "\n",
      "Score: 0.4843583405017853\t \n",
      "Topic: 0.004*\"reader\" + 0.003*\"circuit\" + 0.003*\"generator\" + 0.002*\"traffic\" + 0.002*\"count\" + 0.002*\"sans\" + 0.002*\"communicate\" + 0.002*\"criterion\" + 0.002*\"norm\" + 0.002*\"pddl\" + 0.002*\"rgnn\" + 0.002*\"logistic\" + 0.002*\"boaf\" + 0.002*\"bdcmf\" + 0.002*\"milp\" + 0.002*\"fluid\" + 0.002*\"blind\" + 0.002*\"spot\" + 0.002*\"mtas\" + 0.002*\"create\" + 0.002*\"planner\" + 0.002*\"kernel\" + 0.002*\"vmcqs\" + 0.002*\"scene\" + 0.002*\"activation\" + 0.002*\"subgoals\" + 0.002*\"imvc\" + 0.002*\"independence\" + 0.002*\"bone\" + 0.002*\"molecule\"\n",
      "\n",
      "Topic id: 27\n",
      "\n",
      "Score: 0.33079037070274353\t \n",
      "Topic: 0.006*\"model\" + 0.005*\"learn\" + 0.005*\"network\" + 0.005*\"data\" + 0.005*\"feature\" + 0.004*\"method\" + 0.004*\"image\" + 0.004*\"label\" + 0.004*\"task\" + 0.004*\"agent\" + 0.004*\"algorithm\" + 0.004*\"train\" + 0.004*\"problem\" + 0.004*\"approach\" + 0.004*\"domain\" + 0.004*\"information\" + 0.003*\"representation\" + 0.003*\"neural\" + 0.003*\"base\" + 0.003*\"word\" + 0.003*\"time\" + 0.003*\"graph\" + 0.003*\"multi\" + 0.003*\"text\" + 0.003*\"performance\" + 0.003*\"knowledge\" + 0.003*\"deep\" + 0.003*\"sample\" + 0.003*\"video\" + 0.003*\"structure\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model[corpus_tfidf[hd_sim_doc_id]], key=lambda tup: -1*tup[1]):\n",
    "    print('Topic id:',index)\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\\n\".format(score, lda_model.print_topic(index, 30)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Sample Document : 137 ]\n",
      "\n",
      "Entity Alignment between Knowledge Graphs Using Attribute Embeddings \n",
      "\n",
      "AAAI Technical Track: AI and the Web \n",
      "\n",
      "The task of entity alignment between knowledge graphs aims to find entities in two knowledge graphs that represent the same real-world entity. Recently, embedding-based models are proposed for this task. Such models are built on top of a knowledge graph embedding model that learns entity embeddings to capture the semantic similarity between entities in the same knowledge graph. We propose to learn embeddings that can capture the similarity between entities in different knowledge graphs. Our proposed model helps align entities from different knowledge graphs, and hence enables the integration of multiple knowledge graphs. Our model exploits large numbers of attribute triples existing in the knowledge graphs and generates attribute character embeddings. The attribute character embedding shifts the entity embeddings from two knowledge graphs into the same space by computing the similarity between entities based on their attributes. We use a transitivity rule to further enrich the number of attributes of an entity to enhance the attribute character embedding. Experiments using real-world knowledge bases show that our proposed model achieves consistent improvements over the baseline models by over 50% in terms of hits@1 on the entity alignment task. \n",
      "\n",
      "\n",
      "[ Most similar Document - Cos Similarity : 137 ]\n",
      "\n",
      "Entity Alignment between Knowledge Graphs Using Attribute Embeddings \n",
      "\n",
      "AAAI Technical Track: AI and the Web \n",
      "\n",
      "The task of entity alignment between knowledge graphs aims to find entities in two knowledge graphs that represent the same real-world entity. Recently, embedding-based models are proposed for this task. Such models are built on top of a knowledge graph embedding model that learns entity embeddings to capture the semantic similarity between entities in the same knowledge graph. We propose to learn embeddings that can capture the similarity between entities in different knowledge graphs. Our proposed model helps align entities from different knowledge graphs, and hence enables the integration of multiple knowledge graphs. Our model exploits large numbers of attribute triples existing in the knowledge graphs and generates attribute character embeddings. The attribute character embedding shifts the entity embeddings from two knowledge graphs into the same space by computing the similarity between entities based on their attributes. We use a transitivity rule to further enrich the number of attributes of an entity to enhance the attribute character embedding. Experiments using real-world knowledge bases show that our proposed model achieves consistent improvements over the baseline models by over 50% in terms of hits@1 on the entity alignment task. \n",
      "\n",
      "\n",
      "[ Most similar Document - Hellinger distance : 869 ]\n",
      "\n",
      "Learning Logistic Circuits \n",
      "\n",
      "AAAI Technical Track: Machine Learning \n",
      "\n",
      "This paper proposes a new classification model called logistic circuits. On MNIST and Fashion datasets, our learning algorithm outperforms neural networks that have an order of magnitude more parameters. Yet, logistic circuits have a distinct origin in symbolic AI, forming a discriminative counterpart to probabilistic-logical circuits such as ACs, SPNs, and PSDDs. We show that parameter learning for logistic circuits is convex optimization, and that a simple local search algorithm can induce strong model structures from data. \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "show_doc('Sample Document',sample_doc_id)\n",
    "show_doc('Most similar Document - Cos Similarity',cos_sim_doc_id)\n",
    "show_doc('Most similar Document - Hellinger distance',hd_sim_doc_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
