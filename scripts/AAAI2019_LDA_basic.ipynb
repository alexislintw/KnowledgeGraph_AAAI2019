{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 以LDA模型計算文件相似度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from enum import Enum\n",
    "from pprint import pprint\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from gensim import corpora, models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自定義 data types and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContentType(Enum):\n",
    "    TIT = 'title'\n",
    "    ABS = 'abstract'\n",
    "    AUT = 'author'\n",
    "    SEC = 'section'\n",
    "    \n",
    "def get_contents(content_type):\n",
    "    all_contents = []\n",
    "    dataset_path = '../dataset'\n",
    "    for file in os.listdir(dataset_path):\n",
    "        file_path = os.path.join(dataset_path, file)\n",
    "        if os.path.isfile(file_path):\n",
    "            with open(file_path) as f:  \n",
    "                line = f.readlines()\n",
    "                if content_type == ContentType.AUT:\n",
    "                    line = line[1]\n",
    "                elif content_type == ContentType.SEC:\n",
    "                    line = line[2]\n",
    "                elif content_type == ContentType.ABS:\n",
    "                    line = line[3]\n",
    "                else:\n",
    "                    line = line[0]\n",
    "                line = line.strip()\n",
    "                all_contents.append(line)\n",
    "        else:\n",
    "            print(file_path + ' does not exist.')\n",
    "    return all_contents\n",
    "\n",
    "\n",
    "def get_all_titles():\n",
    "    return get_contents(ContentType.TIT)\n",
    "\n",
    "def get_all_authors():        \n",
    "    return get_contents(ContentType.AUT)\n",
    "\n",
    "def get_all_sections():\n",
    "    return get_contents(ContentType.SEC)\n",
    "\n",
    "def get_all_abstracts():\n",
    "    return get_contents(ContentType.ABS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 預處理\n",
    "- 以gensim的simple_preprocess處理：斷詞，統一小寫，去標點\n",
    "- 去除stopword\n",
    "- lemmatize,stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            token = wordnet_lemmatizer.lemmatize(token, pos='v')\n",
    "            token = wordnet_lemmatizer.lemmatize(token, pos='n')\n",
    "            result.append(token)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 取出所有摘要"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "共 1343 篇論文\n",
      "\n"
     ]
    }
   ],
   "source": [
    "contents = get_all_abstracts()\n",
    "print('共',len(contents),'篇論文\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>We consider the problem of actively eliciting ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>We investigate the task of distractor generati...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The most common representation formalisms for ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Statistical relational learning models are pow...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Multimodal representation learning is gaining ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Reinforcement learning (RL) has shown its adva...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Selecting appropriate tutoring help actions th...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Recognizing time expressions is a fundamental ...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>When facing large-scale image datasets, online...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Temporal modeling in videos is a fundamental y...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            abstract  index\n",
       "0  We consider the problem of actively eliciting ...      0\n",
       "1  We investigate the task of distractor generati...      1\n",
       "2  The most common representation formalisms for ...      2\n",
       "3  Statistical relational learning models are pow...      3\n",
       "4  Multimodal representation learning is gaining ...      4\n",
       "5  Reinforcement learning (RL) has shown its adva...      5\n",
       "6  Selecting appropriate tutoring help actions th...      6\n",
       "7  Recognizing time expressions is a fundamental ...      7\n",
       "8  When facing large-scale image datasets, online...      8\n",
       "9  Temporal modeling in videos is a fundamental y...      9"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = pd.DataFrame(data=contents,columns=['abstract'])\n",
    "documents['index'] = documents.index\n",
    "documents[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 取其中一篇，比較預處理前後"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original document: \n",
      "In real-world applications of natural language generation, there are often constraints on the target sentences in addition to fluency and naturalness requirements. Existing language generation techniques are usually based on recurrent neural networks (RNNs). However, it is non-trivial to impose constraints on RNNs while maintaining generation quality, since RNNs generate sentences sequentially (or with beam search) from the first word to the last. In this paper, we propose CGMH, a novel approach using Metropolis-Hastings sampling for constrained sentence generation. CGMH allows complicated constraints such as the occurrence of multiple keywords in the target sentences, which cannot be handled in traditional RNN-based approaches. Moreover, CGMH works in the inference stage, and does not require parallel corpora for training. We evaluate our method on a variety of tasks, including keywords-to-sentence generation, unsupervised sentence paraphrasing, and unsupervised sentence error correction. CGMH achieves high performance compared with previous supervised methods for sentence generation. Our code is released at https://github.com/NingMiao/CGMH\n",
      "\n",
      "\n",
      "original tokens: \n",
      "['In', 'real-world', 'applications', 'of', 'natural', 'language', 'generation,', 'there', 'are', 'often', 'constraints', 'on', 'the', 'target', 'sentences', 'in', 'addition', 'to', 'fluency', 'and', 'naturalness', 'requirements.', 'Existing', 'language', 'generation', 'techniques', 'are', 'usually', 'based', 'on', 'recurrent', 'neural', 'networks', '(RNNs).', 'However,', 'it', 'is', 'non-trivial', 'to', 'impose', 'constraints', 'on', 'RNNs', 'while', 'maintaining', 'generation', 'quality,', 'since', 'RNNs', 'generate', 'sentences', 'sequentially', '(or', 'with', 'beam', 'search)', 'from', 'the', 'first', 'word', 'to', 'the', 'last.', 'In', 'this', 'paper,', 'we', 'propose', 'CGMH,', 'a', 'novel', 'approach', 'using', 'Metropolis-Hastings', 'sampling', 'for', 'constrained', 'sentence', 'generation.', 'CGMH', 'allows', 'complicated', 'constraints', 'such', 'as', 'the', 'occurrence', 'of', 'multiple', 'keywords', 'in', 'the', 'target', 'sentences,', 'which', 'cannot', 'be', 'handled', 'in', 'traditional', 'RNN-based', 'approaches.', 'Moreover,', 'CGMH', 'works', 'in', 'the', 'inference', 'stage,', 'and', 'does', 'not', 'require', 'parallel', 'corpora', 'for', 'training.', 'We', 'evaluate', 'our', 'method', 'on', 'a', 'variety', 'of', 'tasks,', 'including', 'keywords-to-sentence', 'generation,', 'unsupervised', 'sentence', 'paraphrasing,', 'and', 'unsupervised', 'sentence', 'error', 'correction.', 'CGMH', 'achieves', 'high', 'performance', 'compared', 'with', 'previous', 'supervised', 'methods', 'for', 'sentence', 'generation.', 'Our', 'code', 'is', 'released', 'at', 'https://github.com/NingMiao/CGMH']\n",
      "\n",
      "\n",
      "lemmatized tokens: \n",
      "['real', 'world', 'application', 'natural', 'language', 'generation', 'constraint', 'target', 'sentence', 'addition', 'fluency', 'naturalness', 'requirement', 'exist', 'language', 'generation', 'technique', 'usually', 'base', 'recurrent', 'neural', 'network', 'rnns', 'trivial', 'impose', 'constraint', 'rnns', 'maintain', 'generation', 'quality', 'rnns', 'generate', 'sentence', 'sequentially', 'beam', 'search', 'word', 'paper', 'propose', 'cgmh', 'novel', 'approach', 'metropolis', 'hastings', 'sample', 'constrain', 'sentence', 'generation', 'cgmh', 'allow', 'complicate', 'constraint', 'occurrence', 'multiple', 'keywords', 'target', 'sentence', 'handle', 'traditional', 'base', 'approach', 'cgmh', 'work', 'inference', 'stage', 'require', 'parallel', 'corpus', 'train', 'evaluate', 'method', 'variety', 'task', 'include', 'keywords', 'sentence', 'generation', 'unsupervised', 'sentence', 'paraphrase', 'unsupervised', 'sentence', 'error', 'correction', 'cgmh', 'achieve', 'high', 'performance', 'compare', 'previous', 'supervise', 'method', 'sentence', 'generation', 'code', 'release', 'http', 'github', 'ningmiao', 'cgmh']\n",
      "\n",
      "共 100 字\n"
     ]
    }
   ],
   "source": [
    "sample_doc_id = 1001\n",
    "\n",
    "print('original document: ')\n",
    "print(contents[sample_doc_id])\n",
    "\n",
    "sample_doc = documents[documents['index'] == sample_doc_id].values[0][0]\n",
    "print('\\n\\noriginal tokens: ')\n",
    "\n",
    "words = []\n",
    "for word in sample_doc.split(' '):\n",
    "    words.append(word)\n",
    "print(words)\n",
    "\n",
    "tokens = preprocess(sample_doc)\n",
    "print('\\n\\nlemmatized tokens: ')\n",
    "print(tokens)\n",
    "\n",
    "print('\\n共',len(tokens),'字')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 預處理的全部論文摘要"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [consider, problem, actively, elicit, preferen...\n",
       "1    [investigate, task, distractor, generation, mu...\n",
       "2    [common, representation, formalisms, plan, des...\n",
       "3    [statistical, relational, learn, model, powerf...\n",
       "4    [multimodal, representation, learn, gain, deep...\n",
       "5    [reinforcement, learn, show, advantage, image,...\n",
       "6    [select, appropriate, tutor, help, action, acc...\n",
       "7    [recognize, time, expressions, fundamental, im...\n",
       "8    [face, large, scale, image, datasets, online, ...\n",
       "9    [temporal, model, videos, fundamental, challen...\n",
       "Name: abstract, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs = documents['abstract'].map(preprocess)\n",
    "processed_docs[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 產生bow資料集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 active\n",
      "1 actively\n",
      "2 adaptive\n",
      "3 aggregation\n",
      "4 algorithm\n",
      "5 allow\n",
      "6 approach\n",
      "7 bind\n",
      "8 coefficients\n",
      "9 collective\n",
      "10 combinatorial\n"
     ]
    }
   ],
   "source": [
    "# 產生字典\n",
    "dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "\n",
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 active\n",
      "1 actively\n",
      "2 adaptive\n",
      "3 aggregation\n",
      "4 allow\n",
      "5 bind\n",
      "6 coefficients\n",
      "7 collective\n",
      "8 combinatorial\n",
      "9 context\n",
      "10 control\n"
     ]
    }
   ],
   "source": [
    "# 濾掉出現於少於10篇的字，或是，出現超過半篇的字\n",
    "#dictionary.filter_extremes(no_below=10, no_above=0.4, keep_n=100000)\n",
    "dictionary.filter_extremes(no_above=0.1, keep_n=100000)\n",
    "\n",
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "共 1343 筆\n"
     ]
    }
   ],
   "source": [
    "# 產生 bag of words corpus\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "\n",
    "print('共',len(bow_corpus),'筆')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 查看其中一篇文章(sample document)的bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 4 (\"allow\") appears 1 time.\n",
      "Word 42 (\"supervise\") appears 1 time.\n",
      "Word 62 (\"generation\") appears 6 time.\n",
      "Word 76 (\"recurrent\") appears 1 time.\n",
      "Word 80 (\"sentence\") appears 8 time.\n",
      "Word 90 (\"word\") appears 1 time.\n",
      "Word 133 (\"inference\") appears 1 time.\n",
      "Word 142 (\"parallel\") appears 1 time.\n",
      "Word 167 (\"code\") appears 1 time.\n",
      "Word 179 (\"github\") appears 1 time.\n",
      "Word 181 (\"https\") appears 1 time.\n",
      "Word 187 (\"maintain\") appears 1 time.\n",
      "Word 220 (\"quality\") appears 1 time.\n",
      "Word 282 (\"language\") appears 2 time.\n",
      "Word 286 (\"natural\") appears 1 time.\n",
      "Word 303 (\"addition\") appears 1 time.\n",
      "Word 354 (\"constrain\") appears 1 time.\n",
      "Word 405 (\"target\") appears 2 time.\n",
      "Word 423 (\"handle\") appears 1 time.\n",
      "Word 439 (\"search\") appears 1 time.\n",
      "Word 472 (\"techniques\") appears 1 time.\n",
      "Word 473 (\"usually\") appears 1 time.\n",
      "Word 567 (\"error\") appears 1 time.\n",
      "Word 590 (\"variety\") appears 1 time.\n",
      "Word 809 (\"paraphrase\") appears 1 time.\n",
      "Word 815 (\"traditional\") appears 1 time.\n",
      "Word 922 (\"release\") appears 1 time.\n",
      "Word 985 (\"requirements\") appears 1 time.\n",
      "Word 1028 (\"impose\") appears 1 time.\n",
      "Word 1068 (\"stage\") appears 1 time.\n",
      "Word 1140 (\"corpora\") appears 1 time.\n",
      "Word 1249 (\"rnns\") appears 3 time.\n",
      "Word 1276 (\"constraints\") appears 3 time.\n",
      "Word 1296 (\"unsupervised\") appears 2 time.\n",
      "Word 1305 (\"complicate\") appears 1 time.\n",
      "Word 1688 (\"occurrence\") appears 1 time.\n",
      "Word 1984 (\"trivial\") appears 1 time.\n",
      "Word 2135 (\"metropolis\") appears 1 time.\n",
      "Word 2158 (\"sequentially\") appears 1 time.\n",
      "Word 2166 (\"correction\") appears 1 time.\n",
      "Word 2198 (\"keywords\") appears 2 time.\n",
      "共 61 字\n"
     ]
    }
   ],
   "source": [
    "c = 0\n",
    "\n",
    "sample_doc_bow = bow_corpus[sample_doc_id]\n",
    "for i in range(len(sample_doc_bow)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(sample_doc_bow[i][0], \n",
    "                                               dictionary[sample_doc_bow[i][0]],\n",
    "                                                     sample_doc_bow[i][1]))\n",
    "    c = c + sample_doc_bow[i][1]\n",
    "    \n",
    "print('共',c,'字')   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 產生TF-IDF資料集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4, 0.06068211250153376),\n",
       " (42, 0.05877488379893954),\n",
       " (62, 0.39990753163188436),\n",
       " (76, 0.06964910254609999),\n",
       " (80, 0.5377752767057431),\n",
       " (90, 0.060245133301391904),\n",
       " (133, 0.06609372061495924),\n",
       " (142, 0.092191919964687),\n",
       " (167, 0.08346124545105761),\n",
       " (179, 0.0986356706309067),\n",
       " (181, 0.0986356706309067),\n",
       " (187, 0.07980353556443551),\n",
       " (220, 0.05960408215932745),\n",
       " (282, 0.11401354286626544),\n",
       " (286, 0.059394109510280506),\n",
       " (303, 0.06693490399633174),\n",
       " (354, 0.08710584277208924),\n",
       " (405, 0.11634215221695465),\n",
       " (423, 0.06554871589335923),\n",
       " (439, 0.0642374944326745),\n",
       " (472, 0.06528073869748165),\n",
       " (473, 0.07307049690178523),\n",
       " (567, 0.07234651210975725),\n",
       " (590, 0.08290371079403613),\n",
       " (809, 0.12139029454401536),\n",
       " (815, 0.0738167616122096),\n",
       " (922, 0.0986356706309067),\n",
       " (985, 0.10743674197036336),\n",
       " (1028, 0.09760352807811248),\n",
       " (1068, 0.08915650228883412),\n",
       " (1140, 0.10202513385693786),\n",
       " (1249, 0.317899477403733),\n",
       " (1276, 0.23517124829678981),\n",
       " (1296, 0.14541160593934263),\n",
       " (1305, 0.10458030436493848),\n",
       " (1688, 0.12748509401769942),\n",
       " (1984, 0.11441351825718934),\n",
       " (2135, 0.13564511421509162),\n",
       " (2158, 0.13564511421509162),\n",
       " (2166, 0.12748509401769942),\n",
       " (2198, 0.27129022843018324)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "corpus_tfidf = tfidf[bow_corpus]\n",
    "\n",
    "# sample doc的tf-idf\n",
    "corpus_tfidf[sample_doc_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train LDA models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running LDA using Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaMulticore(bow_corpus, \n",
    "                                       num_topics=num_topics, \n",
    "                                       id2word=dictionary, \n",
    "                                       passes=2, \n",
    "                                       workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0\n",
      "Words: 0.008*\"graph\" + 0.007*\"attack\" + 0.006*\"representations\" + 0.006*\"optimal\" + 0.005*\"latent\" + 0.005*\"sentence\" + 0.005*\"news\" + 0.005*\"point\" + 0.005*\"view\" + 0.005*\"word\"\n",
      "\n",
      "Topic: 1\n",
      "Words: 0.011*\"plan\" + 0.011*\"agents\" + 0.009*\"video\" + 0.009*\"action\" + 0.008*\"agent\" + 0.008*\"noise\" + 0.006*\"local\" + 0.006*\"view\" + 0.006*\"temporal\" + 0.005*\"representations\"\n",
      "\n",
      "Topic: 2\n",
      "Words: 0.011*\"graph\" + 0.008*\"temporal\" + 0.008*\"human\" + 0.008*\"spatial\" + 0.007*\"prediction\" + 0.006*\"sequence\" + 0.005*\"layer\" + 0.005*\"code\" + 0.005*\"search\" + 0.005*\"traffic\"\n",
      "\n",
      "Topic: 3\n",
      "Words: 0.016*\"action\" + 0.014*\"game\" + 0.007*\"agents\" + 0.007*\"question\" + 0.006*\"class\" + 0.006*\"plan\" + 0.006*\"mechanism\" + 0.005*\"answer\" + 0.005*\"value\" + 0.005*\"text\"\n",
      "\n",
      "Topic: 4\n",
      "Words: 0.010*\"decision\" + 0.009*\"agents\" + 0.007*\"agent\" + 0.007*\"user\" + 0.007*\"game\" + 0.007*\"make\" + 0.006*\"value\" + 0.006*\"prediction\" + 0.005*\"sequence\" + 0.005*\"risk\"\n",
      "\n",
      "Topic: 5\n",
      "Words: 0.008*\"decision\" + 0.008*\"plan\" + 0.007*\"agents\" + 0.007*\"hierarchical\" + 0.007*\"layer\" + 0.007*\"group\" + 0.006*\"class\" + 0.005*\"communication\" + 0.005*\"gradient\" + 0.005*\"rule\"\n",
      "\n",
      "Topic: 6\n",
      "Words: 0.009*\"spatial\" + 0.009*\"adversarial\" + 0.007*\"segmentation\" + 0.007*\"weight\" + 0.006*\"resolution\" + 0.005*\"cluster\" + 0.005*\"video\" + 0.005*\"quality\" + 0.005*\"source\" + 0.005*\"frame\"\n",
      "\n",
      "Topic: 7\n",
      "Words: 0.013*\"sentence\" + 0.011*\"word\" + 0.010*\"text\" + 0.009*\"question\" + 0.008*\"language\" + 0.007*\"natural\" + 0.005*\"answer\" + 0.005*\"semantic\" + 0.005*\"distribution\" + 0.005*\"game\"\n",
      "\n",
      "Topic: 8\n",
      "Words: 0.011*\"reward\" + 0.008*\"action\" + 0.007*\"object\" + 0.007*\"graph\" + 0.006*\"joint\" + 0.006*\"context\" + 0.006*\"online\" + 0.005*\"match\" + 0.005*\"agent\" + 0.005*\"representations\"\n",
      "\n",
      "Topic: 9\n",
      "Words: 0.011*\"hash\" + 0.009*\"search\" + 0.009*\"semantic\" + 0.006*\"plan\" + 0.006*\"target\" + 0.006*\"domain\" + 0.005*\"online\" + 0.005*\"cod\" + 0.005*\"spatial\" + 0.004*\"question\"\n",
      "\n",
      "Topic: 10\n",
      "Words: 0.008*\"class\" + 0.008*\"video\" + 0.007*\"event\" + 0.006*\"graph\" + 0.006*\"target\" + 0.006*\"entity\" + 0.005*\"prediction\" + 0.005*\"document\" + 0.005*\"transfer\" + 0.004*\"student\"\n",
      "\n",
      "Topic: 11\n",
      "Words: 0.011*\"semantic\" + 0.010*\"rule\" + 0.008*\"match\" + 0.006*\"visual\" + 0.006*\"sentence\" + 0.006*\"plan\" + 0.005*\"temporal\" + 0.005*\"explanations\" + 0.005*\"compute\" + 0.005*\"detection\"\n",
      "\n",
      "Topic: 12\n",
      "Words: 0.019*\"word\" + 0.010*\"face\" + 0.009*\"embed\" + 0.009*\"user\" + 0.008*\"match\" + 0.008*\"review\" + 0.007*\"human\" + 0.007*\"instance\" + 0.007*\"answer\" + 0.006*\"systems\"\n",
      "\n",
      "Topic: 13\n",
      "Words: 0.014*\"domain\" + 0.009*\"class\" + 0.008*\"bound\" + 0.007*\"relation\" + 0.006*\"word\" + 0.006*\"context\" + 0.006*\"target\" + 0.005*\"language\" + 0.005*\"source\" + 0.004*\"prediction\"\n",
      "\n",
      "Topic: 14\n",
      "Words: 0.017*\"object\" + 0.016*\"shape\" + 0.011*\"attribute\" + 0.010*\"detection\" + 0.009*\"view\" + 0.007*\"instance\" + 0.007*\"cluster\" + 0.006*\"loss\" + 0.006*\"recognition\" + 0.006*\"convolutional\"\n",
      "\n",
      "Topic: 15\n",
      "Words: 0.012*\"text\" + 0.007*\"transfer\" + 0.006*\"detection\" + 0.005*\"word\" + 0.005*\"optimization\" + 0.005*\"target\" + 0.005*\"effect\" + 0.005*\"sense\" + 0.005*\"graph\" + 0.005*\"video\"\n",
      "\n",
      "Topic: 16\n",
      "Words: 0.013*\"sentence\" + 0.013*\"query\" + 0.012*\"language\" + 0.007*\"translation\" + 0.006*\"document\" + 0.006*\"domain\" + 0.006*\"pattern\" + 0.006*\"sequence\" + 0.006*\"natural\" + 0.005*\"generation\"\n",
      "\n",
      "Topic: 17\n",
      "Words: 0.013*\"embed\" + 0.009*\"content\" + 0.009*\"translation\" + 0.009*\"attribute\" + 0.008*\"loss\" + 0.007*\"word\" + 0.007*\"metric\" + 0.007*\"object\" + 0.007*\"table\" + 0.007*\"class\"\n",
      "\n",
      "Topic: 18\n",
      "Words: 0.008*\"embed\" + 0.008*\"graph\" + 0.007*\"cluster\" + 0.007*\"inference\" + 0.006*\"user\" + 0.006*\"answer\" + 0.006*\"nod\" + 0.006*\"matrix\" + 0.006*\"view\" + 0.005*\"approximation\"\n",
      "\n",
      "Topic: 19\n",
      "Words: 0.015*\"domain\" + 0.009*\"target\" + 0.009*\"source\" + 0.008*\"question\" + 0.007*\"domains\" + 0.007*\"test\" + 0.007*\"adversarial\" + 0.007*\"adaptation\" + 0.006*\"representations\" + 0.006*\"unsupervised\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {}\\nWords: {}\\n'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running LDA using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, \n",
    "                                             num_topics=num_topics, \n",
    "                                             id2word=dictionary, \n",
    "                                             passes=2, \n",
    "                                             workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0\n",
      "Word: 0.005*\"domain\" + 0.005*\"search\" + 0.004*\"target\" + 0.004*\"source\" + 0.004*\"rule\" + 0.004*\"metric\" + 0.004*\"review\" + 0.003*\"cluster\" + 0.003*\"domains\" + 0.003*\"spectral\"\n",
      "\n",
      "Topic: 1\n",
      "Word: 0.006*\"graph\" + 0.005*\"vote\" + 0.004*\"word\" + 0.004*\"embed\" + 0.004*\"spatial\" + 0.003*\"question\" + 0.003*\"caption\" + 0.003*\"cost\" + 0.003*\"nod\" + 0.003*\"position\"\n",
      "\n",
      "Topic: 2\n",
      "Word: 0.005*\"sentence\" + 0.004*\"embed\" + 0.004*\"program\" + 0.003*\"answer\" + 0.003*\"representations\" + 0.003*\"risk\" + 0.003*\"word\" + 0.003*\"bound\" + 0.003*\"semantic\" + 0.003*\"spatial\"\n",
      "\n",
      "Topic: 3\n",
      "Word: 0.005*\"face\" + 0.005*\"translation\" + 0.004*\"cluster\" + 0.004*\"hash\" + 0.004*\"online\" + 0.003*\"sequence\" + 0.003*\"workers\" + 0.003*\"agents\" + 0.003*\"game\" + 0.003*\"instance\"\n",
      "\n",
      "Topic: 4\n",
      "Word: 0.004*\"text\" + 0.004*\"policy\" + 0.003*\"document\" + 0.003*\"bayesian\" + 0.003*\"discount\" + 0.003*\"gradient\" + 0.003*\"estimators\" + 0.003*\"experience\" + 0.003*\"approximation\" + 0.003*\"long\"\n",
      "\n",
      "Topic: 5\n",
      "Word: 0.010*\"plan\" + 0.004*\"graph\" + 0.004*\"answer\" + 0.003*\"reason\" + 0.003*\"action\" + 0.003*\"kernel\" + 0.003*\"group\" + 0.003*\"face\" + 0.003*\"linear\" + 0.003*\"agents\"\n",
      "\n",
      "Topic: 6\n",
      "Word: 0.004*\"branch\" + 0.004*\"user\" + 0.003*\"weight\" + 0.003*\"card\" + 0.003*\"behavior\" + 0.003*\"hard\" + 0.003*\"confidence\" + 0.003*\"diagnosis\" + 0.003*\"value\" + 0.003*\"segmentation\"\n",
      "\n",
      "Topic: 7\n",
      "Word: 0.006*\"rank\" + 0.004*\"agent\" + 0.003*\"belief\" + 0.003*\"game\" + 0.003*\"bind\" + 0.003*\"set\" + 0.003*\"speaker\" + 0.003*\"move\" + 0.003*\"videos\" + 0.003*\"concept\"\n",
      "\n",
      "Topic: 8\n",
      "Word: 0.004*\"video\" + 0.004*\"stream\" + 0.004*\"topic\" + 0.003*\"policies\" + 0.003*\"search\" + 0.003*\"point\" + 0.003*\"detection\" + 0.003*\"object\" + 0.003*\"inverse\" + 0.003*\"pattern\"\n",
      "\n",
      "Topic: 9\n",
      "Word: 0.007*\"temporal\" + 0.004*\"human\" + 0.003*\"game\" + 0.003*\"segmentation\" + 0.003*\"resource\" + 0.003*\"skeleton\" + 0.003*\"word\" + 0.003*\"video\" + 0.003*\"quantization\" + 0.003*\"stream\"\n",
      "\n",
      "Topic: 10\n",
      "Word: 0.005*\"action\" + 0.004*\"question\" + 0.004*\"temporal\" + 0.004*\"graph\" + 0.004*\"social\" + 0.003*\"rnns\" + 0.003*\"video\" + 0.003*\"spatial\" + 0.003*\"dependencies\" + 0.003*\"match\"\n",
      "\n",
      "Topic: 11\n",
      "Word: 0.005*\"text\" + 0.004*\"attribute\" + 0.004*\"object\" + 0.004*\"distribution\" + 0.003*\"embed\" + 0.003*\"module\" + 0.003*\"story\" + 0.003*\"target\" + 0.003*\"prediction\" + 0.003*\"context\"\n",
      "\n",
      "Topic: 12\n",
      "Word: 0.006*\"view\" + 0.005*\"class\" + 0.005*\"question\" + 0.004*\"layer\" + 0.004*\"target\" + 0.004*\"answer\" + 0.004*\"word\" + 0.004*\"cluster\" + 0.003*\"domain\" + 0.003*\"reward\"\n",
      "\n",
      "Topic: 13\n",
      "Word: 0.006*\"game\" + 0.004*\"object\" + 0.004*\"program\" + 0.003*\"rule\" + 0.003*\"agent\" + 0.003*\"latent\" + 0.003*\"strategy\" + 0.003*\"properties\" + 0.003*\"search\" + 0.003*\"nod\"\n",
      "\n",
      "Topic: 14\n",
      "Word: 0.005*\"domain\" + 0.004*\"source\" + 0.004*\"summary\" + 0.004*\"adversarial\" + 0.003*\"graph\" + 0.003*\"unsupervised\" + 0.003*\"supervise\" + 0.003*\"video\" + 0.003*\"topic\" + 0.003*\"cycle\"\n",
      "\n",
      "Topic: 15\n",
      "Word: 0.005*\"loss\" + 0.004*\"instance\" + 0.004*\"object\" + 0.004*\"videos\" + 0.003*\"domain\" + 0.003*\"view\" + 0.003*\"online\" + 0.003*\"layer\" + 0.003*\"detection\" + 0.003*\"hierarchical\"\n",
      "\n",
      "Topic: 16\n",
      "Word: 0.004*\"point\" + 0.004*\"message\" + 0.003*\"answer\" + 0.003*\"user\" + 0.003*\"fit\" + 0.003*\"object\" + 0.003*\"connectivity\" + 0.003*\"triple\" + 0.003*\"sequence\" + 0.003*\"speed\"\n",
      "\n",
      "Topic: 17\n",
      "Word: 0.006*\"word\" + 0.005*\"query\" + 0.005*\"sentence\" + 0.005*\"language\" + 0.004*\"graph\" + 0.003*\"expert\" + 0.003*\"embed\" + 0.003*\"english\" + 0.003*\"video\" + 0.003*\"flow\"\n",
      "\n",
      "Topic: 18\n",
      "Word: 0.008*\"plan\" + 0.004*\"action\" + 0.004*\"question\" + 0.004*\"object\" + 0.003*\"energy\" + 0.003*\"detection\" + 0.003*\"items\" + 0.003*\"hide\" + 0.003*\"gradient\" + 0.003*\"value\"\n",
      "\n",
      "Topic: 19\n",
      "Word: 0.004*\"interaction\" + 0.004*\"agents\" + 0.004*\"action\" + 0.004*\"layer\" + 0.004*\"graph\" + 0.004*\"interactions\" + 0.004*\"embed\" + 0.003*\"user\" + 0.003*\"temporal\" + 0.003*\"reinforcement\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {}\\nWord: {}\\n'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance evaluation by classifying sample document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['real', 'world', 'applications', 'natural', 'language', 'generation', 'constraints', 'target', 'sentence', 'addition', 'fluency', 'naturalness', 'requirements', 'exist', 'language', 'generation', 'techniques', 'usually', 'base', 'recurrent', 'neural', 'network', 'rnns', 'trivial', 'impose', 'constraints', 'rnns', 'maintain', 'generation', 'quality', 'rnns', 'generate', 'sentence', 'sequentially', 'beam', 'search', 'word', 'paper', 'propose', 'cgmh', 'novel', 'approach', 'metropolis', 'hastings', 'sample', 'constrain', 'sentence', 'generation', 'cgmh', 'allow', 'complicate', 'constraints', 'occurrence', 'multiple', 'keywords', 'target', 'sentence', 'handle', 'traditional', 'base', 'approach', 'cgmh', 'work', 'inference', 'stage', 'require', 'parallel', 'corpora', 'train', 'evaluate', 'method', 'variety', 'task', 'include', 'keywords', 'sentence', 'generation', 'unsupervised', 'sentence', 'paraphrase', 'unsupervised', 'sentence', 'error', 'correction', 'cgmh', 'achieve', 'high', 'performance', 'compare', 'previous', 'supervise', 'methods', 'sentence', 'generation', 'code', 'release', 'https', 'github', 'ningmiao', 'cgmh']\n"
     ]
    }
   ],
   "source": [
    "print(processed_docs[sample_doc_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic id: 16\n",
      "\n",
      "Score: 0.9846753478050232\t \n",
      "Topic: 0.013*\"sentence\" + 0.013*\"query\" + 0.012*\"language\" + 0.007*\"translation\" + 0.006*\"document\" + 0.006*\"domain\" + 0.006*\"pattern\" + 0.006*\"sequence\" + 0.006*\"natural\" + 0.005*\"generation\" + 0.005*\"word\" + 0.005*\"layer\" + 0.005*\"paraphrase\" + 0.005*\"quality\" + 0.005*\"hide\" + 0.004*\"attack\" + 0.004*\"target\" + 0.004*\"action\" + 0.004*\"human\" + 0.004*\"adversarial\" + 0.004*\"systems\" + 0.004*\"examples\" + 0.004*\"understand\" + 0.004*\"decoder\" + 0.004*\"answer\" + 0.004*\"saliency\" + 0.003*\"video\" + 0.003*\"allow\" + 0.003*\"domains\" + 0.003*\"encoder\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model[bow_corpus[sample_doc_id]], key=lambda tup: -1*tup[1]):\n",
    "    print('Topic id:',index)\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\\n\".format(score, lda_model.print_topic(index, 30)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic id: 1\n",
      "\n",
      "Score: 0.5217251181602478\t \n",
      "Topic: 0.006*\"graph\" + 0.005*\"vote\" + 0.004*\"word\" + 0.004*\"embed\" + 0.004*\"spatial\" + 0.003*\"question\" + 0.003*\"caption\" + 0.003*\"cost\" + 0.003*\"nod\" + 0.003*\"position\" + 0.003*\"class\" + 0.003*\"cluster\" + 0.003*\"bound\" + 0.003*\"abstraction\" + 0.003*\"local\" + 0.003*\"convolutional\" + 0.003*\"optimization\" + 0.003*\"video\" + 0.003*\"convex\" + 0.003*\"sentence\" + 0.003*\"underlie\" + 0.003*\"generation\" + 0.003*\"weight\" + 0.003*\"set\" + 0.002*\"predict\" + 0.002*\"recurrent\" + 0.002*\"inference\" + 0.002*\"dependency\" + 0.002*\"general\" + 0.002*\"decision\"\n",
      "Topic id: 17\n",
      "\n",
      "Score: 0.32334885001182556\t \n",
      "Topic: 0.006*\"word\" + 0.005*\"query\" + 0.005*\"sentence\" + 0.005*\"language\" + 0.004*\"graph\" + 0.003*\"expert\" + 0.003*\"embed\" + 0.003*\"english\" + 0.003*\"video\" + 0.003*\"flow\" + 0.003*\"fair\" + 0.003*\"layer\" + 0.003*\"decision\" + 0.003*\"decoder\" + 0.002*\"loss\" + 0.002*\"margin\" + 0.002*\"paraphrase\" + 0.002*\"attack\" + 0.002*\"attribute\" + 0.002*\"computational\" + 0.002*\"post\" + 0.002*\"context\" + 0.002*\"examples\" + 0.002*\"entity\" + 0.002*\"pair\" + 0.002*\"domain\" + 0.002*\"translation\" + 0.002*\"source\" + 0.002*\"target\" + 0.002*\"visual\"\n",
      "Topic id: 4\n",
      "\n",
      "Score: 0.07727880775928497\t \n",
      "Topic: 0.004*\"text\" + 0.004*\"policy\" + 0.003*\"document\" + 0.003*\"bayesian\" + 0.003*\"discount\" + 0.003*\"gradient\" + 0.003*\"estimators\" + 0.003*\"experience\" + 0.003*\"approximation\" + 0.003*\"long\" + 0.002*\"cluster\" + 0.002*\"agent\" + 0.002*\"sequence\" + 0.002*\"action\" + 0.002*\"saliency\" + 0.002*\"frame\" + 0.002*\"probability\" + 0.002*\"selection\" + 0.002*\"online\" + 0.002*\"students\" + 0.002*\"range\" + 0.002*\"allow\" + 0.002*\"deal\" + 0.002*\"intelligence\" + 0.002*\"select\" + 0.002*\"supervise\" + 0.002*\"round\" + 0.002*\"objective\" + 0.002*\"distance\" + 0.002*\"widely\"\n",
      "Topic id: 3\n",
      "\n",
      "Score: 0.06471161544322968\t \n",
      "Topic: 0.005*\"face\" + 0.005*\"translation\" + 0.004*\"cluster\" + 0.004*\"hash\" + 0.004*\"online\" + 0.003*\"sequence\" + 0.003*\"workers\" + 0.003*\"agents\" + 0.003*\"game\" + 0.003*\"instance\" + 0.003*\"detection\" + 0.003*\"reinforcement\" + 0.003*\"embed\" + 0.003*\"content\" + 0.003*\"class\" + 0.003*\"facial\" + 0.003*\"message\" + 0.002*\"agent\" + 0.002*\"sentiment\" + 0.002*\"exploration\" + 0.002*\"graph\" + 0.002*\"quality\" + 0.002*\"users\" + 0.002*\"document\" + 0.002*\"test\" + 0.002*\"similarity\" + 0.002*\"anomalies\" + 0.002*\"human\" + 0.002*\"word\" + 0.002*\"expert\"\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model_tfidf[bow_corpus[sample_doc_id]], key=lambda tup: -1*tup[1]):\n",
    "    print('Topic id:',index)\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model_tfidf.print_topic(index, 30)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 找出最相似的文件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 以sample document為例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 方式一：cos相似\n",
    "cos_sim = []\n",
    "vec_lda1 = lda_model[bow_corpus[sample_doc_id]]\n",
    "for i in range(len(bow_corpus)):\n",
    "    vec_lda2 = lda_model[bow_corpus[i]]\n",
    "    sim = gensim.matutils.cossim(vec_lda1, vec_lda2)\n",
    "    cos_sim.append(sim)\n",
    "\n",
    "top2_sim_index = sorted(range(len(cos_sim)), key=lambda i: cos_sim[i])[-2:]\n",
    "cos_most_sim_id = top2_sim_index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 方式二：Hellinger distance is useful for similarity between probability distributions (such as LDA topics)\n",
    "hd_sim = []\n",
    "vec_lda1 = lda_model[bow_corpus[sample_doc_id]]\n",
    "for i in range(len(bow_corpus)):\n",
    "    vec_lda2 = lda_model[bow_corpus[i]]\n",
    "    dense1 = gensim.matutils.sparse2full(vec_lda1, lda_model.num_topics)\n",
    "    dense2 = gensim.matutils.sparse2full(vec_lda2, lda_model.num_topics)\n",
    "    sim = np.sqrt(0.5 * ((np.sqrt(dense1) - np.sqrt(dense2))**2).sum())\n",
    "    hd_sim.append(sim)\n",
    "\n",
    "top1_sim_index = sorted(range(len(hd_sim)), key=lambda i: hd_sim[i])[-1:]\n",
    "hd_most_sim_id = top1_sim_index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = get_all_titles()\n",
    "sections = get_all_sections()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_doc(head,doc_id):\n",
    "    print('[',head,':',doc_id,']\\n')\n",
    "    print(titles[doc_id],'\\n')\n",
    "    print(sections[doc_id],'\\n')\n",
    "    print(contents[doc_id],'\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Sample Document : 1001 ]\n",
      "\n",
      "CGMH: Constrained Sentence Generation by Metropolis-Hastings Sampling \n",
      "\n",
      "AAAI Technical Track: Natural Language Processing \n",
      "\n",
      "In real-world applications of natural language generation, there are often constraints on the target sentences in addition to fluency and naturalness requirements. Existing language generation techniques are usually based on recurrent neural networks (RNNs). However, it is non-trivial to impose constraints on RNNs while maintaining generation quality, since RNNs generate sentences sequentially (or with beam search) from the first word to the last. In this paper, we propose CGMH, a novel approach using Metropolis-Hastings sampling for constrained sentence generation. CGMH allows complicated constraints such as the occurrence of multiple keywords in the target sentences, which cannot be handled in traditional RNN-based approaches. Moreover, CGMH works in the inference stage, and does not require parallel corpora for training. We evaluate our method on a variety of tasks, including keywords-to-sentence generation, unsupervised sentence paraphrasing, and unsupervised sentence error correction. CGMH achieves high performance compared with previous supervised methods for sentence generation. Our code is released at https://github.com/NingMiao/CGMH \n",
      "\n",
      "\n",
      "[ Most similar Document(Cos Similarity) : 510 ]\n",
      "\n",
      "Graph Based Translation Memory for Neural Machine Translation \n",
      "\n",
      "AAAI Technical Track: Natural Language Processing \n",
      "\n",
      "A translation memory (TM) is proved to be helpful to improve neural machine translation (NMT). Existing approaches either pursue the decoding efficiency by merely accessing local information in a TM or encode the global information in a TM yet sacrificing efficiency due to redundancy. We propose an efficient approach to making use of the global information in a TM. The key idea is to pack a redundant TM into a compact graph and perform additional attention mechanisms over the packed graph for integrating the TM representation into the decoding network. We implement the model by extending the state-of-the-art NMT, Transformer. Extensive experiments on three language pairs show that the proposed approach is efficient in terms of running time and space occupation, and particularly it outperforms multiple strong baselines in terms of BLEU scores. \n",
      "\n",
      "\n",
      "[ Most similar Document(Hellinger distance) : 178 ]\n",
      "\n",
      "ATP: Directed Graph Embedding with Asymmetric Transitivity Preservation \n",
      "\n",
      "AAAI Technical Track: AI and the Web \n",
      "\n",
      "Directed graphs have been widely used in Community Question Answering services (CQAs) to model asymmetric relationships among different types of nodes in CQA graphs, e.g., question, answer, user. Asymmetric transitivity is an essential property of directed graphs, since it can play an important role in downstream graph inference and analysis. Question difficulty and user expertise follow the characteristic of asymmetric transitivity. Maintaining such properties, while reducing the graph to a lower dimensional vector embedding space, has been the focus of much recent research. In this paper, we tackle the challenge of directed graph embedding with asymmetric transitivity preservation and then leverage the proposed embedding method to solve a fundamental task in CQAs: how to appropriately route and assign newly posted questions to users with the suitable expertise and interest in CQAs. The technique incorporates graph hierarchy and reachability information naturally by relying on a nonlinear transformation that operates on the core reachability and implicit hierarchy within such graphs. Subsequently, the methodology levers a factorization-based approach to generate two embedding vectors for each node within the graph, to capture the asymmetric transitivity. Extensive experiments show that our framework consistently and significantly outperforms the state-of-the-art baselines on three diverse realworld tasks: link prediction, and question difficulty estimation and expert finding in online forums like Stack Exchange. Particularly, our framework can support inductive embedding learning for newly posted questions (unseen nodes during training), and therefore can properly route and assign these kinds of questions to experts in CQAs. \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#documents.loc[sample_doc_id,'abstract']\n",
    "\n",
    "show_doc('Sample Document',sample_doc_id)\n",
    "show_doc('Most similar Document(Cos Similarity)',cos_most_sim_id)\n",
    "show_doc('Most similar Document(Hellinger distance)',hd_most_sim_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### small test\n",
    "- gensim.utils.simple_preprocess(doc,deacc=True)\n",
    "- deacc=True 是去除上標，不是去除標點符號，預設就已經會去除標點符號了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['in', 'real', 'world', 'applications', 'of', 'natural', 'language', 'generation', 'there', 'are', 'often', 'constraints', 'on', 'the', 'target', 'sentences', 'in', 'addition', 'to', 'fluency', 'and', 'naturalness', 'requirements', 'existing', 'language', 'generation', 'techniques', 'are', 'usually', 'based', 'on', 'recurrent', 'neural', 'networks', 'rnns', 'however', 'it', 'is', 'non', 'trivial', 'to', 'impose', 'constraints', 'on', 'rnns', 'while', 'maintaining', 'generation', 'quality', 'since', 'rnns', 'generate', 'sentences', 'sequentially', 'or', 'with', 'beam', 'search', 'from', 'the', 'first', 'word', 'to', 'the', 'last', 'in', 'this', 'paper', 'we', 'propose', 'cgmh', 'novel', 'approach', 'using', 'metropolis', 'hastings', 'sampling', 'for', 'constrained', 'sentence', 'generation', 'cgmh', 'allows', 'complicated', 'constraints', 'such', 'as', 'the', 'occurrence', 'of', 'multiple', 'keywords', 'in', 'the', 'target', 'sentences', 'which', 'cannot', 'be', 'handled', 'in', 'traditional', 'rnn', 'based', 'approaches', 'moreover', 'cgmh', 'works', 'in', 'the', 'inference', 'stage', 'and', 'does', 'not', 'require', 'parallel', 'corpora', 'for', 'training', 'we', 'evaluate', 'our', 'method', 'on', 'variety', 'of', 'tasks', 'including', 'keywords', 'to', 'sentence', 'generation', 'unsupervised', 'sentence', 'paraphrasing', 'and', 'unsupervised', 'sentence', 'error', 'correction', 'cgmh', 'achieves', 'high', 'performance', 'compared', 'with', 'previous', 'supervised', 'methods', 'for', 'sentence', 'generation', 'our', 'code', 'is', 'released', 'at', 'https', 'github', 'com', 'ningmiao', 'cgmh']\n"
     ]
    }
   ],
   "source": [
    "s = 'In real-world applications of natural language generation, there are often constraints on the target sentences in addition to fluency and naturalness requirements. Existing language generation techniques are usually based on recurrent neural networks (RNNs). However, it is non-trivial to impose constraints on RNNs while maintaining generation quality, since RNNs generate sentences sequentially (or with beam search) from the first word to the last. In this paper, we propose CGMH, a novel approach using Metropolis-Hastings sampling for constrained sentence generation. CGMH allows complicated constraints such as the occurrence of multiple keywords in the target sentences, which cannot be handled in traditional RNN-based approaches. Moreover, CGMH works in the inference stage, and does not require parallel corpora for training. We evaluate our method on a variety of tasks, including keywords-to-sentence generation, unsupervised sentence paraphrasing, and unsupervised sentence error correction. CGMH achieves high performance compared with previous supervised methods for sentence generation. Our code is released at https://github.com/NingMiao/CGMH'\n",
    "r = []\n",
    "for token in gensim.utils.simple_preprocess(s,deacc=True):#,deacc=True\n",
    "    r.append(token)\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
