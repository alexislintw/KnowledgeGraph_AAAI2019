{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 以LDA模型計算文件相似度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from enum import Enum\n",
    "from pprint import pprint\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import nltk\n",
    "\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from gensim import corpora, models\n",
    "\n",
    "# 初始化\n",
    "porter_stemmer = PorterStemmer()\n",
    "snowball_stemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自定義 data types and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContentType(Enum):\n",
    "    TIT = 'title'\n",
    "    ABS = 'abstract'\n",
    "    AUT = 'author'\n",
    "    SEC = 'section'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_contents(content_type):\n",
    "    all_contents = []\n",
    "    dataset_path = '../dataset'\n",
    "    for file in os.listdir(dataset_path):\n",
    "        file_path = os.path.join(dataset_path, file)\n",
    "        if os.path.isfile(file_path):\n",
    "            with open(file_path) as f:  \n",
    "                line = f.readlines()\n",
    "                if content_type == ContentType.AUT:\n",
    "                    line = line[1]\n",
    "                elif content_type == ContentType.SEC:\n",
    "                    line = line[2]\n",
    "                elif content_type == ContentType.ABS:\n",
    "                    line = line[3]\n",
    "                else:\n",
    "                    line = line[0]\n",
    "                line = line.strip()\n",
    "                all_contents.append(line)\n",
    "        else:\n",
    "            print(file_path + ' does not exist.')\n",
    "    return all_contents\n",
    "\n",
    "\n",
    "def get_all_titles():\n",
    "    return get_contents(ContentType.TIT)\n",
    "\n",
    "def get_all_authors():        \n",
    "    return get_contents(ContentType.AUT)\n",
    "\n",
    "def get_all_abstracts():\n",
    "    return get_contents(ContentType.ABS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 預處理\n",
    "- 以gensim的simple_preprocess處理：斷詞，統一小寫，去標點\n",
    "- 去除stopword\n",
    "- lemmatize,stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_stemming(text):\n",
    "    #return porter_stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "    return WordNetLemmatizer().lemmatize(text, pos='v')\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 取出所有摘要"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "共 1343 篇論文\n",
      "\n"
     ]
    }
   ],
   "source": [
    "contents = get_all_abstracts()\n",
    "print('共',len(contents),'篇論文\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>We consider the problem of actively eliciting ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>We investigate the task of distractor generati...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The most common representation formalisms for ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Statistical relational learning models are pow...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Multimodal representation learning is gaining ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Reinforcement learning (RL) has shown its adva...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Selecting appropriate tutoring help actions th...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Recognizing time expressions is a fundamental ...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>When facing large-scale image datasets, online...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Temporal modeling in videos is a fundamental y...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            abstract  index\n",
       "0  We consider the problem of actively eliciting ...      0\n",
       "1  We investigate the task of distractor generati...      1\n",
       "2  The most common representation formalisms for ...      2\n",
       "3  Statistical relational learning models are pow...      3\n",
       "4  Multimodal representation learning is gaining ...      4\n",
       "5  Reinforcement learning (RL) has shown its adva...      5\n",
       "6  Selecting appropriate tutoring help actions th...      6\n",
       "7  Recognizing time expressions is a fundamental ...      7\n",
       "8  When facing large-scale image datasets, online...      8\n",
       "9  Temporal modeling in videos is a fundamental y...      9"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = pd.DataFrame(data=contents,columns=['abstract'])\n",
    "documents['index'] = documents.index\n",
    "documents[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 取其中一篇，比較預處理前後"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original document: \n",
      "In real-world applications of natural language generation, there are often constraints on the target sentences in addition to fluency and naturalness requirements. Existing language generation techniques are usually based on recurrent neural networks (RNNs). However, it is non-trivial to impose constraints on RNNs while maintaining generation quality, since RNNs generate sentences sequentially (or with beam search) from the first word to the last. In this paper, we propose CGMH, a novel approach using Metropolis-Hastings sampling for constrained sentence generation. CGMH allows complicated constraints such as the occurrence of multiple keywords in the target sentences, which cannot be handled in traditional RNN-based approaches. Moreover, CGMH works in the inference stage, and does not require parallel corpora for training. We evaluate our method on a variety of tasks, including keywords-to-sentence generation, unsupervised sentence paraphrasing, and unsupervised sentence error correction. CGMH achieves high performance compared with previous supervised methods for sentence generation. Our code is released at https://github.com/NingMiao/CGMH\n",
      "\n",
      "\n",
      "original tokens: \n",
      "['In', 'real-world', 'applications', 'of', 'natural', 'language', 'generation,', 'there', 'are', 'often', 'constraints', 'on', 'the', 'target', 'sentences', 'in', 'addition', 'to', 'fluency', 'and', 'naturalness', 'requirements.', 'Existing', 'language', 'generation', 'techniques', 'are', 'usually', 'based', 'on', 'recurrent', 'neural', 'networks', '(RNNs).', 'However,', 'it', 'is', 'non-trivial', 'to', 'impose', 'constraints', 'on', 'RNNs', 'while', 'maintaining', 'generation', 'quality,', 'since', 'RNNs', 'generate', 'sentences', 'sequentially', '(or', 'with', 'beam', 'search)', 'from', 'the', 'first', 'word', 'to', 'the', 'last.', 'In', 'this', 'paper,', 'we', 'propose', 'CGMH,', 'a', 'novel', 'approach', 'using', 'Metropolis-Hastings', 'sampling', 'for', 'constrained', 'sentence', 'generation.', 'CGMH', 'allows', 'complicated', 'constraints', 'such', 'as', 'the', 'occurrence', 'of', 'multiple', 'keywords', 'in', 'the', 'target', 'sentences,', 'which', 'cannot', 'be', 'handled', 'in', 'traditional', 'RNN-based', 'approaches.', 'Moreover,', 'CGMH', 'works', 'in', 'the', 'inference', 'stage,', 'and', 'does', 'not', 'require', 'parallel', 'corpora', 'for', 'training.', 'We', 'evaluate', 'our', 'method', 'on', 'a', 'variety', 'of', 'tasks,', 'including', 'keywords-to-sentence', 'generation,', 'unsupervised', 'sentence', 'paraphrasing,', 'and', 'unsupervised', 'sentence', 'error', 'correction.', 'CGMH', 'achieves', 'high', 'performance', 'compared', 'with', 'previous', 'supervised', 'methods', 'for', 'sentence', 'generation.', 'Our', 'code', 'is', 'released', 'at', 'https://github.com/NingMiao/CGMH']\n",
      "\n",
      "\n",
      "lemmatized tokens: \n",
      "['real', 'world', 'applications', 'natural', 'language', 'generation', 'constraints', 'target', 'sentence', 'addition', 'fluency', 'naturalness', 'requirements', 'exist', 'language', 'generation', 'techniques', 'usually', 'base', 'recurrent', 'neural', 'network', 'rnns', 'trivial', 'impose', 'constraints', 'rnns', 'maintain', 'generation', 'quality', 'rnns', 'generate', 'sentence', 'sequentially', 'beam', 'search', 'word', 'paper', 'propose', 'cgmh', 'novel', 'approach', 'metropolis', 'hastings', 'sample', 'constrain', 'sentence', 'generation', 'cgmh', 'allow', 'complicate', 'constraints', 'occurrence', 'multiple', 'keywords', 'target', 'sentence', 'handle', 'traditional', 'base', 'approach', 'cgmh', 'work', 'inference', 'stage', 'require', 'parallel', 'corpora', 'train', 'evaluate', 'method', 'variety', 'task', 'include', 'keywords', 'sentence', 'generation', 'unsupervised', 'sentence', 'paraphrase', 'unsupervised', 'sentence', 'error', 'correction', 'cgmh', 'achieve', 'high', 'performance', 'compare', 'previous', 'supervise', 'methods', 'sentence', 'generation', 'code', 'release', 'https', 'github', 'ningmiao', 'cgmh']\n",
      "\n",
      "共 100 字\n"
     ]
    }
   ],
   "source": [
    "sample_doc_id = 1001\n",
    "\n",
    "print('original document: ')\n",
    "print(contents[sample_doc_id])\n",
    "\n",
    "sample_doc = documents[documents['index'] == sample_doc_id].values[0][0]\n",
    "print('\\n\\noriginal tokens: ')\n",
    "\n",
    "words = []\n",
    "for word in sample_doc.split(' '):\n",
    "    words.append(word)\n",
    "print(words)\n",
    "\n",
    "tokens = preprocess(sample_doc)\n",
    "print('\\n\\nlemmatized tokens: ')\n",
    "print(tokens)\n",
    "\n",
    "print('\\n共',len(tokens),'字')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 預處理的全部論文摘要"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [consider, problem, actively, elicit, preferen...\n",
       "1    [investigate, task, distractor, generation, mu...\n",
       "2    [common, representation, formalisms, plan, des...\n",
       "3    [statistical, relational, learn, model, powerf...\n",
       "4    [multimodal, representation, learn, gain, deep...\n",
       "5    [reinforcement, learn, show, advantage, image,...\n",
       "6    [select, appropriate, tutor, help, action, acc...\n",
       "7    [recognize, time, expressions, fundamental, im...\n",
       "8    [face, large, scale, image, datasets, online, ...\n",
       "9    [temporal, model, videos, fundamental, challen...\n",
       "Name: abstract, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs = documents['abstract'].map(preprocess)\n",
    "processed_docs[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 產生bow資料集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 active\n",
      "1 actively\n",
      "2 adaptive\n",
      "3 aggregation\n",
      "4 algorithm\n",
      "5 allow\n",
      "6 approach\n",
      "7 bind\n",
      "8 coefficients\n",
      "9 collective\n",
      "10 combinatorial\n"
     ]
    }
   ],
   "source": [
    "# 產生字典\n",
    "dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "\n",
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 active\n",
      "1 adaptive\n",
      "2 aggregation\n",
      "3 algorithm\n",
      "4 allow\n",
      "5 approach\n",
      "6 bind\n",
      "7 combinatorial\n",
      "8 compare\n",
      "9 consider\n",
      "10 context\n"
     ]
    }
   ],
   "source": [
    "# 濾掉出現於少於10篇的字，或是，出現超過半篇的字\n",
    "dictionary.filter_extremes(no_below=10, no_above=0.4, keep_n=100000)\n",
    "\n",
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "共 1343 筆\n"
     ]
    }
   ],
   "source": [
    "# 產生 bag of words corpus\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "\n",
    "print('共',len(bow_corpus),'筆')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 查看其中一篇文章(sample document)的bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 4 (\"allow\") appears 1 time.\n",
      "Word 5 (\"approach\") appears 2 time.\n",
      "Word 8 (\"compare\") appears 1 time.\n",
      "Word 48 (\"supervise\") appears 1 time.\n",
      "Word 68 (\"generate\") appears 1 time.\n",
      "Word 69 (\"generation\") appears 6 time.\n",
      "Word 80 (\"multiple\") appears 1 time.\n",
      "Word 84 (\"previous\") appears 1 time.\n",
      "Word 87 (\"real\") appears 1 time.\n",
      "Word 88 (\"recurrent\") appears 1 time.\n",
      "Word 92 (\"sentence\") appears 8 time.\n",
      "Word 101 (\"task\") appears 1 time.\n",
      "Word 104 (\"word\") appears 1 time.\n",
      "Word 105 (\"work\") appears 1 time.\n",
      "Word 121 (\"require\") appears 1 time.\n",
      "Word 149 (\"inference\") appears 1 time.\n",
      "Word 157 (\"method\") appears 1 time.\n",
      "Word 161 (\"parallel\") appears 1 time.\n",
      "Word 181 (\"world\") appears 1 time.\n",
      "Word 187 (\"code\") appears 1 time.\n",
      "Word 202 (\"github\") appears 1 time.\n",
      "Word 204 (\"https\") appears 1 time.\n",
      "Word 209 (\"maintain\") appears 1 time.\n",
      "Word 243 (\"methods\") appears 1 time.\n",
      "Word 245 (\"quality\") appears 1 time.\n",
      "Word 291 (\"achieve\") appears 1 time.\n",
      "Word 292 (\"applications\") appears 1 time.\n",
      "Word 303 (\"exist\") appears 1 time.\n",
      "Word 312 (\"language\") appears 2 time.\n",
      "Word 316 (\"natural\") appears 1 time.\n",
      "Word 319 (\"performance\") appears 1 time.\n",
      "Word 329 (\"train\") appears 1 time.\n",
      "Word 333 (\"addition\") appears 1 time.\n",
      "Word 358 (\"novel\") appears 1 time.\n",
      "Word 384 (\"constrain\") appears 1 time.\n",
      "Word 390 (\"include\") appears 1 time.\n",
      "Word 392 (\"network\") appears 1 time.\n",
      "Word 413 (\"evaluate\") appears 1 time.\n",
      "Word 418 (\"neural\") appears 1 time.\n",
      "Word 430 (\"target\") appears 2 time.\n",
      "Word 449 (\"handle\") appears 1 time.\n",
      "Word 450 (\"high\") appears 1 time.\n",
      "Word 464 (\"search\") appears 1 time.\n",
      "Word 497 (\"sample\") appears 1 time.\n",
      "Word 499 (\"techniques\") appears 1 time.\n",
      "Word 500 (\"usually\") appears 1 time.\n",
      "Word 573 (\"error\") appears 1 time.\n",
      "Word 593 (\"variety\") appears 1 time.\n",
      "Word 787 (\"traditional\") appears 1 time.\n",
      "Word 870 (\"release\") appears 1 time.\n",
      "Word 912 (\"requirements\") appears 1 time.\n",
      "Word 946 (\"impose\") appears 1 time.\n",
      "Word 979 (\"stage\") appears 1 time.\n",
      "Word 1031 (\"corpora\") appears 1 time.\n",
      "Word 1103 (\"rnns\") appears 3 time.\n",
      "Word 1122 (\"constraints\") appears 3 time.\n",
      "Word 1135 (\"unsupervised\") appears 2 time.\n",
      "Word 1138 (\"complicate\") appears 1 time.\n",
      "Word 1456 (\"trivial\") appears 1 time.\n",
      "共 79 字\n"
     ]
    }
   ],
   "source": [
    "c = 0\n",
    "\n",
    "sample_doc_bow = bow_corpus[sample_doc_id]\n",
    "for i in range(len(sample_doc_bow)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(sample_doc_bow[i][0], \n",
    "                                               dictionary[sample_doc_bow[i][0]],\n",
    "                                                     sample_doc_bow[i][1]))\n",
    "    c = c + sample_doc_bow[i][1]\n",
    "    \n",
    "print('共',c,'字')   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 產生TF-IDF資料集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4, 0.06456533739403816),\n",
       " (5, 0.05277587385667208),\n",
       " (8, 0.04913897524372444),\n",
       " (48, 0.062536059578974),\n",
       " (68, 0.04598531720453427),\n",
       " (69, 0.4254987778412788),\n",
       " (80, 0.05200141170584894),\n",
       " (84, 0.057795388137199435),\n",
       " (87, 0.04036477998522042),\n",
       " (88, 0.07410615121494404),\n",
       " (92, 0.5721890809552969),\n",
       " (101, 0.028386635685164065),\n",
       " (104, 0.06410039462378403),\n",
       " (105, 0.03621715719381838),\n",
       " (121, 0.051572532708930974),\n",
       " (149, 0.07032325005205249),\n",
       " (157, 0.03144752146410322),\n",
       " (161, 0.09809154909321473),\n",
       " (181, 0.04862799566264723),\n",
       " (187, 0.0888021733214704),\n",
       " (202, 0.10494765410829669),\n",
       " (204, 0.10494765410829669),\n",
       " (209, 0.0849103959395723),\n",
       " (243, 0.029994301765549916),\n",
       " (245, 0.06341832075443449),\n",
       " (291, 0.03606124357370797),\n",
       " (292, 0.053181211510352364),\n",
       " (303, 0.03278133908202476),\n",
       " (312, 0.1213096011195062),\n",
       " (316, 0.06319491134480174),\n",
       " (319, 0.030932718840479326),\n",
       " (329, 0.032105812921030846),\n",
       " (333, 0.07121826320485267),\n",
       " (358, 0.033195309869213596),\n",
       " (384, 0.09267999902656376),\n",
       " (390, 0.0569085783307159),\n",
       " (392, 0.026441415964311536),\n",
       " (413, 0.05605123615827533),\n",
       " (418, 0.03824998947209109),\n",
       " (430, 0.12378722495605869),\n",
       " (449, 0.06974336889299494),\n",
       " (450, 0.04992493323039434),\n",
       " (464, 0.06834823855692972),\n",
       " (497, 0.05505866664873469),\n",
       " (499, 0.0694582430568549),\n",
       " (500, 0.07774649054768057),\n",
       " (573, 0.07697617586286701),\n",
       " (593, 0.08820896039997643),\n",
       " (787, 0.07854051090767702),\n",
       " (870, 0.10494765410829669),\n",
       " (912, 0.11431193160352489),\n",
       " (946, 0.10384946175122907),\n",
       " (979, 0.09486188621079095),\n",
       " (1031, 0.10855401894551055),\n",
       " (1103, 0.33824278967614463),\n",
       " (1122, 0.25022054054686343),\n",
       " (1135, 0.15471691758004108),\n",
       " (1138, 0.11127270224685716),\n",
       " (1456, 0.12173517209915298)]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "corpus_tfidf = tfidf[bow_corpus]\n",
    "\n",
    "# sample doc的tf-idf\n",
    "corpus_tfidf[sample_doc_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train LDA models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running LDA using Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaMulticore(bow_corpus, \n",
    "                                       num_topics=num_topics, \n",
    "                                       id2word=dictionary, \n",
    "                                       passes=2, \n",
    "                                       workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0\n",
      "Words: 0.018*\"word\" + 0.012*\"network\" + 0.011*\"train\" + 0.011*\"data\" + 0.010*\"approach\" + 0.010*\"neural\" + 0.007*\"deep\" + 0.007*\"sample\" + 0.006*\"state\" + 0.006*\"performance\"\n",
      "\n",
      "Topic: 1\n",
      "Words: 0.026*\"network\" + 0.015*\"embed\" + 0.015*\"view\" + 0.012*\"structure\" + 0.012*\"data\" + 0.012*\"action\" + 0.010*\"space\" + 0.009*\"function\" + 0.009*\"approach\" + 0.008*\"graph\"\n",
      "\n",
      "Topic: 2\n",
      "Words: 0.018*\"task\" + 0.016*\"feature\" + 0.013*\"network\" + 0.012*\"text\" + 0.010*\"word\" + 0.010*\"level\" + 0.009*\"neural\" + 0.009*\"state\" + 0.009*\"approach\" + 0.008*\"information\"\n",
      "\n",
      "Topic: 3\n",
      "Words: 0.020*\"data\" + 0.009*\"feature\" + 0.008*\"information\" + 0.008*\"user\" + 0.007*\"performance\" + 0.007*\"methods\" + 0.007*\"recommendation\" + 0.006*\"task\" + 0.006*\"work\" + 0.006*\"algorithms\"\n",
      "\n",
      "Topic: 4\n",
      "Words: 0.011*\"task\" + 0.010*\"approach\" + 0.010*\"methods\" + 0.010*\"framework\" + 0.009*\"train\" + 0.008*\"user\" + 0.007*\"problems\" + 0.007*\"algorithm\" + 0.007*\"plan\" + 0.007*\"problem\"\n",
      "\n",
      "Topic: 5\n",
      "Words: 0.015*\"task\" + 0.014*\"network\" + 0.010*\"question\" + 0.009*\"neural\" + 0.009*\"problem\" + 0.008*\"method\" + 0.008*\"structure\" + 0.008*\"performance\" + 0.008*\"sequence\" + 0.008*\"experiment\"\n",
      "\n",
      "Topic: 6\n",
      "Words: 0.018*\"network\" + 0.010*\"information\" + 0.010*\"performance\" + 0.009*\"methods\" + 0.009*\"neural\" + 0.009*\"train\" + 0.008*\"knowledge\" + 0.008*\"program\" + 0.007*\"state\" + 0.007*\"feature\"\n",
      "\n",
      "Topic: 7\n",
      "Words: 0.014*\"train\" + 0.014*\"data\" + 0.013*\"game\" + 0.012*\"network\" + 0.009*\"prediction\" + 0.008*\"generate\" + 0.008*\"human\" + 0.007*\"feature\" + 0.007*\"process\" + 0.007*\"real\"\n",
      "\n",
      "Topic: 8\n",
      "Words: 0.021*\"data\" + 0.015*\"train\" + 0.015*\"network\" + 0.010*\"method\" + 0.007*\"relation\" + 0.007*\"detection\" + 0.007*\"algorithm\" + 0.007*\"object\" + 0.007*\"neural\" + 0.006*\"methods\"\n",
      "\n",
      "Topic: 9\n",
      "Words: 0.015*\"method\" + 0.013*\"data\" + 0.010*\"state\" + 0.010*\"domain\" + 0.009*\"methods\" + 0.009*\"target\" + 0.009*\"problem\" + 0.008*\"performance\" + 0.008*\"approach\" + 0.007*\"answer\"\n",
      "\n",
      "Topic: 10\n",
      "Words: 0.031*\"image\" + 0.021*\"network\" + 0.012*\"approach\" + 0.009*\"generate\" + 0.007*\"train\" + 0.007*\"video\" + 0.007*\"temporal\" + 0.007*\"method\" + 0.006*\"process\" + 0.006*\"neural\"\n",
      "\n",
      "Topic: 11\n",
      "Words: 0.014*\"approach\" + 0.014*\"cluster\" + 0.009*\"data\" + 0.009*\"deep\" + 0.009*\"state\" + 0.009*\"information\" + 0.008*\"latent\" + 0.008*\"structure\" + 0.007*\"market\" + 0.006*\"network\"\n",
      "\n",
      "Topic: 12\n",
      "Words: 0.014*\"data\" + 0.011*\"method\" + 0.011*\"network\" + 0.009*\"content\" + 0.009*\"methods\" + 0.008*\"problem\" + 0.008*\"information\" + 0.007*\"sample\" + 0.007*\"approach\" + 0.007*\"exist\"\n",
      "\n",
      "Topic: 13\n",
      "Words: 0.016*\"image\" + 0.013*\"task\" + 0.012*\"problem\" + 0.011*\"network\" + 0.010*\"object\" + 0.009*\"feature\" + 0.009*\"agent\" + 0.008*\"methods\" + 0.008*\"multi\" + 0.007*\"exist\"\n",
      "\n",
      "Topic: 14\n",
      "Words: 0.029*\"task\" + 0.014*\"rule\" + 0.011*\"sense\" + 0.009*\"decision\" + 0.008*\"train\" + 0.007*\"introduce\" + 0.006*\"methods\" + 0.006*\"problem\" + 0.006*\"algorithm\" + 0.006*\"word\"\n",
      "\n",
      "Topic: 15\n",
      "Words: 0.016*\"label\" + 0.013*\"data\" + 0.013*\"feature\" + 0.012*\"approach\" + 0.010*\"state\" + 0.009*\"train\" + 0.008*\"plan\" + 0.008*\"task\" + 0.007*\"problem\" + 0.006*\"class\"\n",
      "\n",
      "Topic: 16\n",
      "Words: 0.012*\"approach\" + 0.012*\"structure\" + 0.010*\"problem\" + 0.007*\"state\" + 0.007*\"knowledge\" + 0.007*\"attention\" + 0.007*\"sentence\" + 0.006*\"network\" + 0.006*\"reward\" + 0.006*\"video\"\n",
      "\n",
      "Topic: 17\n",
      "Words: 0.024*\"data\" + 0.019*\"feature\" + 0.013*\"label\" + 0.012*\"network\" + 0.012*\"train\" + 0.009*\"method\" + 0.009*\"methods\" + 0.008*\"graph\" + 0.008*\"noise\" + 0.007*\"datasets\"\n",
      "\n",
      "Topic: 18\n",
      "Words: 0.018*\"time\" + 0.013*\"approach\" + 0.012*\"data\" + 0.012*\"network\" + 0.008*\"algorithm\" + 0.008*\"problem\" + 0.007*\"task\" + 0.006*\"train\" + 0.006*\"method\" + 0.006*\"performance\"\n",
      "\n",
      "Topic: 19\n",
      "Words: 0.016*\"face\" + 0.014*\"methods\" + 0.013*\"hash\" + 0.012*\"attribute\" + 0.012*\"label\" + 0.010*\"method\" + 0.010*\"exist\" + 0.009*\"image\" + 0.008*\"information\" + 0.008*\"problem\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {}\\nWords: {}\\n'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running LDA using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, \n",
    "                                             num_topics=num_topics, \n",
    "                                             id2word=dictionary, \n",
    "                                             passes=2, \n",
    "                                             workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0\n",
      "Word: 0.006*\"feature\" + 0.005*\"temporal\" + 0.004*\"spatial\" + 0.004*\"network\" + 0.004*\"data\" + 0.004*\"attention\" + 0.004*\"methods\" + 0.004*\"point\" + 0.004*\"human\" + 0.003*\"class\"\n",
      "\n",
      "Topic: 1\n",
      "Word: 0.005*\"market\" + 0.005*\"frame\" + 0.004*\"image\" + 0.004*\"game\" + 0.004*\"community\" + 0.003*\"train\" + 0.003*\"multi\" + 0.003*\"data\" + 0.003*\"condition\" + 0.003*\"agents\"\n",
      "\n",
      "Topic: 2\n",
      "Word: 0.005*\"causal\" + 0.004*\"face\" + 0.004*\"decision\" + 0.004*\"action\" + 0.004*\"word\" + 0.004*\"task\" + 0.004*\"objective\" + 0.004*\"agent\" + 0.004*\"graph\" + 0.003*\"data\"\n",
      "\n",
      "Topic: 3\n",
      "Word: 0.004*\"word\" + 0.004*\"network\" + 0.004*\"satisfiability\" + 0.004*\"check\" + 0.004*\"domains\" + 0.004*\"text\" + 0.004*\"domain\" + 0.004*\"change\" + 0.004*\"task\" + 0.004*\"post\"\n",
      "\n",
      "Topic: 4\n",
      "Word: 0.006*\"time\" + 0.005*\"network\" + 0.004*\"document\" + 0.004*\"game\" + 0.004*\"graph\" + 0.004*\"algorithms\" + 0.004*\"word\" + 0.003*\"rat\" + 0.003*\"image\" + 0.003*\"search\"\n",
      "\n",
      "Topic: 5\n",
      "Word: 0.009*\"hash\" + 0.007*\"vote\" + 0.005*\"network\" + 0.005*\"rank\" + 0.004*\"binary\" + 0.004*\"modal\" + 0.004*\"rule\" + 0.004*\"tree\" + 0.004*\"data\" + 0.004*\"label\"\n",
      "\n",
      "Topic: 6\n",
      "Word: 0.008*\"image\" + 0.006*\"view\" + 0.005*\"shape\" + 0.005*\"data\" + 0.005*\"network\" + 0.004*\"different\" + 0.004*\"transfer\" + 0.004*\"fair\" + 0.004*\"group\" + 0.004*\"style\"\n",
      "\n",
      "Topic: 7\n",
      "Word: 0.005*\"rule\" + 0.005*\"content\" + 0.004*\"data\" + 0.004*\"object\" + 0.004*\"group\" + 0.004*\"class\" + 0.004*\"users\" + 0.003*\"objective\" + 0.003*\"point\" + 0.003*\"representations\"\n",
      "\n",
      "Topic: 8\n",
      "Word: 0.005*\"sentence\" + 0.004*\"transition\" + 0.004*\"strategy\" + 0.004*\"reward\" + 0.004*\"evolutionary\" + 0.004*\"tree\" + 0.004*\"students\" + 0.004*\"action\" + 0.004*\"plan\" + 0.004*\"search\"\n",
      "\n",
      "Topic: 9\n",
      "Word: 0.006*\"feature\" + 0.006*\"word\" + 0.005*\"question\" + 0.005*\"information\" + 0.005*\"knowledge\" + 0.004*\"image\" + 0.004*\"translation\" + 0.004*\"text\" + 0.004*\"projection\" + 0.004*\"video\"\n",
      "\n",
      "Topic: 10\n",
      "Word: 0.007*\"image\" + 0.005*\"object\" + 0.005*\"network\" + 0.005*\"generate\" + 0.005*\"generation\" + 0.004*\"accuracy\" + 0.004*\"sentence\" + 0.004*\"scale\" + 0.003*\"deep\" + 0.003*\"layer\"\n",
      "\n",
      "Topic: 11\n",
      "Word: 0.004*\"temporal\" + 0.004*\"feature\" + 0.004*\"data\" + 0.004*\"sentence\" + 0.004*\"adversarial\" + 0.004*\"label\" + 0.004*\"instance\" + 0.004*\"neural\" + 0.004*\"shape\" + 0.003*\"network\"\n",
      "\n",
      "Topic: 12\n",
      "Word: 0.005*\"agents\" + 0.005*\"plan\" + 0.004*\"optimization\" + 0.004*\"cluster\" + 0.004*\"stochastic\" + 0.004*\"bandit\" + 0.004*\"agent\" + 0.004*\"data\" + 0.004*\"pose\" + 0.003*\"algorithms\"\n",
      "\n",
      "Topic: 13\n",
      "Word: 0.008*\"label\" + 0.008*\"cluster\" + 0.005*\"graph\" + 0.004*\"network\" + 0.004*\"object\" + 0.004*\"train\" + 0.004*\"multi\" + 0.004*\"layer\" + 0.003*\"method\" + 0.003*\"constraint\"\n",
      "\n",
      "Topic: 14\n",
      "Word: 0.006*\"graph\" + 0.005*\"video\" + 0.005*\"class\" + 0.004*\"data\" + 0.004*\"representations\" + 0.004*\"human\" + 0.004*\"improve\" + 0.004*\"triple\" + 0.004*\"sentence\" + 0.003*\"language\"\n",
      "\n",
      "Topic: 15\n",
      "Word: 0.005*\"attack\" + 0.004*\"candidate\" + 0.004*\"story\" + 0.004*\"train\" + 0.004*\"quality\" + 0.003*\"translation\" + 0.003*\"text\" + 0.003*\"information\" + 0.003*\"strategies\" + 0.003*\"high\"\n",
      "\n",
      "Topic: 16\n",
      "Word: 0.005*\"image\" + 0.005*\"source\" + 0.005*\"target\" + 0.005*\"network\" + 0.005*\"domain\" + 0.004*\"resolution\" + 0.004*\"method\" + 0.004*\"detection\" + 0.004*\"information\" + 0.004*\"feature\"\n",
      "\n",
      "Topic: 17\n",
      "Word: 0.008*\"plan\" + 0.005*\"network\" + 0.005*\"multi\" + 0.005*\"question\" + 0.005*\"approach\" + 0.004*\"reason\" + 0.004*\"answer\" + 0.004*\"feature\" + 0.004*\"problem\" + 0.004*\"algorithms\"\n",
      "\n",
      "Topic: 18\n",
      "Word: 0.006*\"query\" + 0.005*\"social\" + 0.004*\"function\" + 0.004*\"approach\" + 0.004*\"present\" + 0.004*\"agents\" + 0.004*\"data\" + 0.004*\"diagnosis\" + 0.003*\"control\" + 0.003*\"systems\"\n",
      "\n",
      "Topic: 19\n",
      "Word: 0.005*\"action\" + 0.005*\"agent\" + 0.005*\"train\" + 0.004*\"answer\" + 0.004*\"space\" + 0.004*\"domain\" + 0.004*\"sample\" + 0.004*\"data\" + 0.004*\"domains\" + 0.004*\"temporal\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {}\\nWord: {}\\n'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance evaluation by classifying sample document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['real', 'world', 'applications', 'natural', 'language', 'generation', 'constraints', 'target', 'sentence', 'addition', 'fluency', 'naturalness', 'requirements', 'exist', 'language', 'generation', 'techniques', 'usually', 'base', 'recurrent', 'neural', 'network', 'rnns', 'trivial', 'impose', 'constraints', 'rnns', 'maintain', 'generation', 'quality', 'rnns', 'generate', 'sentence', 'sequentially', 'beam', 'search', 'word', 'paper', 'propose', 'cgmh', 'novel', 'approach', 'metropolis', 'hastings', 'sample', 'constrain', 'sentence', 'generation', 'cgmh', 'allow', 'complicate', 'constraints', 'occurrence', 'multiple', 'keywords', 'target', 'sentence', 'handle', 'traditional', 'base', 'approach', 'cgmh', 'work', 'inference', 'stage', 'require', 'parallel', 'corpora', 'train', 'evaluate', 'method', 'variety', 'task', 'include', 'keywords', 'sentence', 'generation', 'unsupervised', 'sentence', 'paraphrase', 'unsupervised', 'sentence', 'error', 'correction', 'cgmh', 'achieve', 'high', 'performance', 'compare', 'previous', 'supervise', 'methods', 'sentence', 'generation', 'code', 'release', 'https', 'github', 'ningmiao', 'cgmh']\n"
     ]
    }
   ],
   "source": [
    "print(processed_docs[sample_doc_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic id: 18\n",
      "\n",
      "Score: 0.619809091091156\t \n",
      "Topic: 0.015*\"task\" + 0.012*\"attention\" + 0.012*\"word\" + 0.011*\"train\" + 0.009*\"approach\" + 0.009*\"video\" + 0.009*\"state\" + 0.009*\"data\" + 0.008*\"sentence\" + 0.008*\"information\" + 0.006*\"network\" + 0.006*\"sequence\" + 0.006*\"question\" + 0.006*\"outperform\" + 0.006*\"novel\" + 0.006*\"method\" + 0.006*\"temporal\" + 0.006*\"structure\" + 0.006*\"problem\" + 0.006*\"generate\" + 0.006*\"work\" + 0.005*\"answer\" + 0.005*\"neural\" + 0.005*\"datasets\" + 0.005*\"experiment\" + 0.005*\"improve\" + 0.005*\"language\" + 0.005*\"target\" + 0.005*\"different\" + 0.004*\"embed\"\n",
      "\n",
      "Topic id: 13\n",
      "\n",
      "Score: 0.25408852100372314\t \n",
      "Topic: 0.013*\"data\" + 0.012*\"approach\" + 0.012*\"network\" + 0.010*\"prediction\" + 0.009*\"performance\" + 0.008*\"sample\" + 0.008*\"feature\" + 0.008*\"state\" + 0.007*\"generate\" + 0.007*\"time\" + 0.006*\"problem\" + 0.006*\"neural\" + 0.006*\"process\" + 0.005*\"structure\" + 0.005*\"datasets\" + 0.005*\"methods\" + 0.005*\"temporal\" + 0.005*\"high\" + 0.005*\"demonstrate\" + 0.005*\"improve\" + 0.004*\"study\" + 0.004*\"traffic\" + 0.004*\"train\" + 0.004*\"task\" + 0.004*\"achieve\" + 0.004*\"novel\" + 0.004*\"predict\" + 0.004*\"experiment\" + 0.004*\"multi\" + 0.004*\"real\"\n",
      "\n",
      "Topic id: 6\n",
      "\n",
      "Score: 0.11547569930553436\t \n",
      "Topic: 0.018*\"task\" + 0.013*\"problem\" + 0.010*\"multi\" + 0.008*\"performance\" + 0.008*\"present\" + 0.007*\"constraints\" + 0.007*\"agent\" + 0.007*\"reason\" + 0.006*\"bandit\" + 0.006*\"approach\" + 0.006*\"bound\" + 0.006*\"train\" + 0.006*\"demonstrate\" + 0.006*\"function\" + 0.006*\"type\" + 0.006*\"algorithm\" + 0.005*\"agents\" + 0.005*\"state\" + 0.005*\"study\" + 0.005*\"weight\" + 0.005*\"time\" + 0.005*\"different\" + 0.005*\"knowledge\" + 0.005*\"achieve\" + 0.005*\"problems\" + 0.005*\"data\" + 0.005*\"order\" + 0.005*\"work\" + 0.005*\"case\" + 0.005*\"algorithms\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model[bow_corpus[sample_doc_id]], key=lambda tup: -1*tup[1]):\n",
    "    print('Topic id:',index)\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\\n\".format(score, lda_model.print_topic(index, 30)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic id: 11\n",
      "\n",
      "Score: 0.5399236083030701\t \n",
      "Topic: 0.004*\"temporal\" + 0.004*\"feature\" + 0.004*\"data\" + 0.004*\"sentence\" + 0.004*\"adversarial\" + 0.004*\"label\" + 0.004*\"instance\" + 0.004*\"neural\" + 0.004*\"shape\" + 0.003*\"network\" + 0.003*\"framework\" + 0.003*\"sample\" + 0.003*\"examples\" + 0.003*\"algorithm\" + 0.003*\"work\" + 0.003*\"match\" + 0.003*\"time\" + 0.003*\"scale\" + 0.003*\"general\" + 0.003*\"gradient\" + 0.003*\"problems\" + 0.003*\"detection\" + 0.003*\"relations\" + 0.003*\"train\" + 0.003*\"task\" + 0.003*\"distribution\" + 0.003*\"approach\" + 0.003*\"perturbations\" + 0.003*\"view\" + 0.003*\"different\"\n",
      "Topic id: 17\n",
      "\n",
      "Score: 0.19564417004585266\t \n",
      "Topic: 0.008*\"plan\" + 0.005*\"network\" + 0.005*\"multi\" + 0.005*\"question\" + 0.005*\"approach\" + 0.004*\"reason\" + 0.004*\"answer\" + 0.004*\"feature\" + 0.004*\"problem\" + 0.004*\"algorithms\" + 0.004*\"video\" + 0.004*\"task\" + 0.004*\"deep\" + 0.004*\"information\" + 0.004*\"classification\" + 0.003*\"train\" + 0.003*\"knowledge\" + 0.003*\"method\" + 0.003*\"agent\" + 0.003*\"score\" + 0.003*\"level\" + 0.003*\"action\" + 0.003*\"modal\" + 0.003*\"neurons\" + 0.003*\"study\" + 0.003*\"reinforcement\" + 0.003*\"search\" + 0.003*\"layer\" + 0.003*\"label\" + 0.003*\"dataset\"\n",
      "Topic id: 8\n",
      "\n",
      "Score: 0.1596209853887558\t \n",
      "Topic: 0.005*\"sentence\" + 0.004*\"transition\" + 0.004*\"strategy\" + 0.004*\"reward\" + 0.004*\"evolutionary\" + 0.004*\"tree\" + 0.004*\"students\" + 0.004*\"action\" + 0.004*\"plan\" + 0.004*\"search\" + 0.003*\"time\" + 0.003*\"label\" + 0.003*\"deep\" + 0.003*\"transfer\" + 0.003*\"methods\" + 0.003*\"game\" + 0.003*\"generation\" + 0.003*\"distance\" + 0.003*\"leave\" + 0.003*\"access\" + 0.003*\"task\" + 0.003*\"sequence\" + 0.003*\"approach\" + 0.003*\"vector\" + 0.003*\"consider\" + 0.003*\"problem\" + 0.003*\"process\" + 0.003*\"reinforcement\" + 0.003*\"function\" + 0.003*\"loss\"\n",
      "Topic id: 13\n",
      "\n",
      "Score: 0.09480295330286026\t \n",
      "Topic: 0.008*\"label\" + 0.008*\"cluster\" + 0.005*\"graph\" + 0.004*\"network\" + 0.004*\"object\" + 0.004*\"train\" + 0.004*\"multi\" + 0.004*\"layer\" + 0.003*\"method\" + 0.003*\"constraint\" + 0.003*\"sequence\" + 0.003*\"algorithms\" + 0.003*\"sample\" + 0.003*\"nod\" + 0.003*\"algorithm\" + 0.003*\"task\" + 0.003*\"latent\" + 0.003*\"supervision\" + 0.003*\"localization\" + 0.003*\"number\" + 0.003*\"time\" + 0.003*\"approach\" + 0.003*\"real\" + 0.003*\"problem\" + 0.003*\"consider\" + 0.003*\"branch\" + 0.003*\"output\" + 0.003*\"data\" + 0.003*\"supervise\" + 0.003*\"weak\"\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model_tfidf[bow_corpus[sample_doc_id]], key=lambda tup: -1*tup[1]):\n",
    "    print('Topic id:',index)\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model_tfidf.print_topic(index, 30)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 找出相似的文件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 以sample document為例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
